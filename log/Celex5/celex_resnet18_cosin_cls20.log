nohup: ignoring input
main.py:110: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
n_per_node: 8
gpu 6
Use GPU: 6 for training
create model mf294
block: 12544, cnn-drop 0.0000, mlp-drop 0.0000
block: 12544, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 24, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 40, token: 192
block: 3136, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 40, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 40, token: 192
block: 3136, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 40, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 72, token: 192
block: 784, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 72, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 72, token: 192
block: 784, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 72, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 128, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 128, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 128, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 128, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 176, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 176, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 176, token: 192
block: 196, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 176, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 240, token: 192
block: 49, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 240, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 240, token: 192
block: 49, cnn-drop 0.0000, mlp-drop 0.0000
L2G: 2 heads, inp: 240, token: 192
G2G: 4 heads
use ffn
G2L: 2 heads, inp: 240, token: 192
L2G: 2 heads, inp: 240, token: 192
MobileFormer(
  (tokens): Embedding(6, 192)
  (stem): Sequential(
    (0): Conv3d(128, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
    (1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (resnet18_feature_extractor): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (features): Sequential(
    (0): DnaBlock3(
      (conv): Sequential(
        (0): Conv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=192, bias=False)
        (1): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Sequential()
        (4): Conv3d(384, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (5): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): DnaBlock3(
      (conv1): Sequential(
        (0): Conv3d(24, 144, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=24, bias=False)
        (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=288, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(144, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv3d(40, 160, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=40, bias=False)
        (1): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=320, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv3d(160, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=24, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=24, bias=True)
        (proj): Linear(in_features=24, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=40, bias=True)
        (proj): Linear(in_features=192, out_features=40, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (2): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(40, 120, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=240, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(120, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=120, bias=False)
        (1): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=240, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(120, 40, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=40, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=40, bias=True)
        (proj): Linear(in_features=40, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=40, bias=True)
        (proj): Linear(in_features=192, out_features=40, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (3): DnaBlock3(
      (conv1): Sequential(
        (0): Conv3d(40, 240, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=40, bias=False)
        (1): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=480, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(240, 72, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv3d(72, 288, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=72, bias=False)
        (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=576, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv3d(288, 72, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=40, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=40, bias=True)
        (proj): Linear(in_features=40, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=72, bias=True)
        (proj): Linear(in_features=192, out_features=72, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (4): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(72, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=432, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (1): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=432, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(216, 72, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=72, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=72, bias=True)
        (proj): Linear(in_features=72, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=72, bias=True)
        (proj): Linear(in_features=192, out_features=72, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (5): DnaBlock3(
      (conv1): Sequential(
        (0): Conv3d(72, 432, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=72, bias=False)
        (1): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=864, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(432, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv3d(128, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128, bias=False)
        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1024, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=72, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=72, bias=True)
        (proj): Linear(in_features=72, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=192, out_features=128, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (6): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1024, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512, bias=False)
        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1024, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=128, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=192, out_features=128, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (7): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(128, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=768, bias=False)
        (1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1536, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(768, 176, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=128, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=128, bias=True)
        (proj): Linear(in_features=128, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=176, bias=True)
        (proj): Linear(in_features=192, out_features=176, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (8): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(176, 1056, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2112, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(1056, 1056, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1056, bias=False)
        (1): BatchNorm3d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2112, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(1056, 176, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=176, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=176, bias=True)
        (proj): Linear(in_features=176, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=176, bias=True)
        (proj): Linear(in_features=192, out_features=176, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (9): DnaBlock3(
      (conv1): Sequential(
        (0): Conv3d(176, 1056, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=176, bias=False)
        (1): BatchNorm3d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2112, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(1056, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (conv3): Sequential(
        (0): Conv3d(240, 960, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=240, bias=False)
        (1): BatchNorm3d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=1920, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv4): Sequential(
        (0): Conv3d(960, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act4): DyReLU(
        (act): Sequential()
      )
      (hyper4): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=176, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=176, bias=True)
        (proj): Linear(in_features=176, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=240, bias=True)
        (proj): Linear(in_features=192, out_features=240, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (10): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(240, 1440, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2880, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(1440, 1440, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1440, bias=False)
        (1): BatchNorm3d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2880, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(1440, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=240, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=240, bias=True)
        (proj): Linear(in_features=240, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=240, bias=True)
        (proj): Linear(in_features=192, out_features=240, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
    (11): DnaBlock(
      (conv1): Sequential(
        (0): Conv3d(240, 1440, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act1): DyReLU(
        (act): Sequential()
      )
      (hyper1): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2880, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv2): Sequential(
        (0): Conv3d(1440, 1440, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=1440, bias=False)
        (1): BatchNorm3d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (act2): DyReLU(
        (act): Sequential()
      )
      (hyper2): HyperFunc(
        (hyper): Sequential(
          (0): Linear(in_features=192, out_features=48, bias=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=48, out_features=2880, bias=True)
          (3): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
      )
      (conv3): Sequential(
        (0): Conv3d(1440, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (1): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Sequential()
      )
      (act3): DyReLU(
        (act): Sequential()
      )
      (hyper3): Sequential()
      (drop_path): DropPath(drop_prob=0.000)
      (local_global): Local2Global(
        (alpha): Sequential(
          (0): Linear(in_features=192, out_features=240, bias=True)
          (1): h_sigmoid(
            (relu): ReLU6(inplace=True)
          )
        )
        (q): Linear(in_features=192, out_features=240, bias=True)
        (proj): Linear(in_features=240, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_block): GlobalBlock(
        (ffn): Sequential(
          (0): Linear(in_features=192, out_features=384, bias=True)
          (1): GELU()
          (2): Linear(in_features=384, out_features=192, bias=True)
        )
        (ffn_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (q): Linear(in_features=192, out_features=192, bias=True)
        (channel_mlp): Linear(in_features=192, out_features=192, bias=True)
        (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
      (global_local): Global2Local(
        (k): Linear(in_features=192, out_features=240, bias=True)
        (proj): Linear(in_features=192, out_features=240, bias=True)
        (drop_path): DropPath(drop_prob=0.000)
      )
    )
  )
  (local_global): Local2Global(
    (alpha): Sequential(
      (0): Linear(in_features=192, out_features=240, bias=True)
      (1): h_sigmoid(
        (relu): ReLU6(inplace=True)
      )
    )
    (q): Linear(in_features=192, out_features=240, bias=True)
    (proj): Linear(in_features=240, out_features=192, bias=True)
    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (drop_path): DropPath(drop_prob=0.000)
  )
  (classifier): MergeClassifier(
    (conv): Sequential(
      (0): Sequential()
      (1): Conv3d(240, 1440, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      (2): BatchNorm3d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (act): DyReLU(
      (act): ReLU6(inplace=True)
    )
    (hyper): Sequential()
    (avgpool): Sequential(
      (0): AdaptiveAvgPool3d(output_size=(1, 1, 1))
      (1): h_swish(
        (sigmoid): h_sigmoid(
          (relu): ReLU6(inplace=True)
        )
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=1632, out_features=1920, bias=True)
      (1): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): h_swish(
        (sigmoid): h_sigmoid(
          (relu): ReLU6(inplace=True)
        )
      )
    )
    (classifier): Sequential(
      (0): Dropout(p=0.3, inplace=False)
      (1): Linear(in_features=1920, out_features=20, bias=True)
    )
  )
)
############################### Dataset loading ###############################
/home/amax/anaconda3/envs/yuan/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
###############################  Dataset loaded  ##############################
Epoch: [0][  0/812]	Time 11.081 (11.081)	Data  3.581 ( 3.581)	Loss 3.0562458038330078 (3.0562458038330078)	Acc@1   6.25 (  6.25)	Acc@5  37.50 ( 37.50)
Epoch: [0][ 10/812]	Time  0.461 ( 1.324)	Data  0.000 ( 0.326)	Loss 3.1944086551666260 (3.5001260584050957)	Acc@1   0.00 (  4.55)	Acc@5  31.25 ( 27.84)
Epoch: [0][ 20/812]	Time  0.244 ( 1.049)	Data  0.000 ( 0.171)	Loss 3.1046490669250488 (3.3911091146014987)	Acc@1   0.00 (  4.17)	Acc@5  31.25 ( 25.60)
Epoch: [0][ 30/812]	Time  0.301 ( 0.818)	Data  0.000 ( 0.116)	Loss 3.5558798313140869 (3.3591227992888419)	Acc@1   6.25 (  5.24)	Acc@5  25.00 ( 27.42)
Epoch: [0][ 40/812]	Time  0.277 ( 0.691)	Data  0.000 ( 0.088)	Loss 3.0488448143005371 (3.2867425011425482)	Acc@1   6.25 (  6.71)	Acc@5  43.75 ( 29.73)
Epoch: [0][ 50/812]	Time  0.717 ( 0.657)	Data  0.001 ( 0.071)	Loss 2.8551399707794189 (3.2551143730387970)	Acc@1  12.50 (  7.60)	Acc@5  43.75 ( 30.39)
Epoch: [0][ 60/812]	Time  0.294 ( 0.627)	Data  0.000 ( 0.059)	Loss 3.1378772258758545 (3.2138011846386019)	Acc@1  18.75 (  7.27)	Acc@5  31.25 ( 31.76)
Epoch: [0][ 70/812]	Time  0.305 ( 0.611)	Data  0.000 ( 0.051)	Loss 3.1072692871093750 (3.2019489315194143)	Acc@1   0.00 (  7.22)	Acc@5  37.50 ( 32.04)
Epoch: [0][ 80/812]	Time  0.277 ( 0.580)	Data  0.000 ( 0.045)	Loss 3.0975120067596436 (3.1882499853769937)	Acc@1   0.00 (  6.94)	Acc@5  12.50 ( 31.87)
Epoch: [0][ 90/812]	Time  0.294 ( 0.552)	Data  0.000 ( 0.040)	Loss 2.7371237277984619 (3.1714882667248068)	Acc@1  12.50 (  6.94)	Acc@5  56.25 ( 32.42)
Epoch: [0][100/812]	Time  0.332 ( 0.553)	Data  0.000 ( 0.036)	Loss 2.8667445182800293 (3.1510130910590144)	Acc@1   6.25 (  7.61)	Acc@5  50.00 ( 33.73)
Epoch: [0][110/812]	Time  0.376 ( 0.567)	Data  0.000 ( 0.033)	Loss 3.2566242218017578 (3.1408184541238322)	Acc@1  12.50 (  7.77)	Acc@5  31.25 ( 33.95)
Epoch: [0][120/812]	Time  0.495 ( 0.545)	Data  0.000 ( 0.030)	Loss 3.0405430793762207 (3.1473999082549544)	Acc@1  12.50 (  7.59)	Acc@5  56.25 ( 33.78)
Epoch: [0][130/812]	Time  0.327 ( 0.539)	Data  0.000 ( 0.028)	Loss 3.1037037372589111 (3.1487260119605609)	Acc@1  18.75 (  8.02)	Acc@5  37.50 ( 34.26)
Epoch: [0][140/812]	Time  0.393 ( 0.541)	Data  0.000 ( 0.026)	Loss 3.3493702411651611 (3.1433235330784575)	Acc@1   0.00 (  8.33)	Acc@5  18.75 ( 34.57)
Epoch: [0][150/812]	Time  0.705 ( 0.559)	Data  0.001 ( 0.024)	Loss 3.5363194942474365 (3.1344931330901900)	Acc@1   6.25 (  8.61)	Acc@5  25.00 ( 35.18)
Epoch: [0][160/812]	Time  0.280 ( 0.552)	Data  0.000 ( 0.023)	Loss 3.2542026042938232 (3.1274397950735153)	Acc@1   6.25 (  8.46)	Acc@5  25.00 ( 34.98)
Epoch: [0][170/812]	Time  0.446 ( 0.541)	Data  0.000 ( 0.021)	Loss 3.3160669803619385 (3.1211620269463078)	Acc@1   6.25 (  8.48)	Acc@5  37.50 ( 35.56)
Epoch: [0][180/812]	Time  0.403 ( 0.558)	Data  0.000 ( 0.020)	Loss 2.7918372154235840 (3.1174557749079077)	Acc@1  18.75 (  8.43)	Acc@5  43.75 ( 35.91)
Epoch: [0][190/812]	Time  0.336 ( 0.546)	Data  0.000 ( 0.019)	Loss 2.7872343063354492 (3.1143690628531093)	Acc@1  18.75 (  8.48)	Acc@5  50.00 ( 36.03)
Epoch: [0][200/812]	Time  0.285 ( 0.534)	Data  0.000 ( 0.018)	Loss 3.0701005458831787 (3.1161754238071726)	Acc@1  12.50 (  8.33)	Acc@5  37.50 ( 36.04)
Epoch: [0][210/812]	Time  0.423 ( 0.532)	Data  0.000 ( 0.017)	Loss 3.3920376300811768 (3.1188441949998031)	Acc@1  12.50 (  8.26)	Acc@5  31.25 ( 35.78)
Epoch: [0][220/812]	Time  0.294 ( 0.524)	Data  0.000 ( 0.017)	Loss 3.3978645801544189 (3.1169060118058147)	Acc@1   6.25 (  8.31)	Acc@5  43.75 ( 36.17)
Epoch: [0][230/812]	Time  0.288 ( 0.513)	Data  0.000 ( 0.016)	Loss 2.8405804634094238 (3.1193282036554244)	Acc@1  25.00 (  8.36)	Acc@5  37.50 ( 36.15)
Epoch: [0][240/812]	Time  0.603 ( 0.510)	Data  0.000 ( 0.015)	Loss 2.8516559600830078 (3.1158796088824134)	Acc@1  18.75 (  8.30)	Acc@5  37.50 ( 36.23)
Epoch: [0][250/812]	Time  0.451 ( 0.511)	Data  0.001 ( 0.015)	Loss 2.6201994419097900 (3.1100225705074598)	Acc@1  18.75 (  8.39)	Acc@5  50.00 ( 36.55)
Epoch: [0][260/812]	Time  0.850 ( 0.514)	Data  0.001 ( 0.014)	Loss 3.2351913452148438 (3.1135456224054212)	Acc@1   6.25 (  8.38)	Acc@5  37.50 ( 36.57)
Epoch: [0][270/812]	Time  0.277 ( 0.512)	Data  0.000 ( 0.014)	Loss 2.8831171989440918 (3.1086037220550198)	Acc@1   6.25 (  8.37)	Acc@5  62.50 ( 36.69)
Epoch: [0][280/812]	Time  0.308 ( 0.507)	Data  0.000 ( 0.013)	Loss 3.3285603523254395 (3.1114088097501056)	Acc@1   0.00 (  8.23)	Acc@5  37.50 ( 36.74)
Epoch: [0][290/812]	Time  0.460 ( 0.501)	Data  0.000 ( 0.013)	Loss 3.2350072860717773 (3.1107451284873937)	Acc@1  25.00 (  8.33)	Acc@5  37.50 ( 36.86)
Epoch: [0][300/812]	Time  0.402 ( 0.510)	Data  0.000 ( 0.012)	Loss 3.2367398738861084 (3.1038329537920779)	Acc@1   6.25 (  8.49)	Acc@5  43.75 ( 37.29)
Epoch: [0][310/812]	Time  0.387 ( 0.507)	Data  0.000 ( 0.012)	Loss 2.8127877712249756 (3.0974032403571812)	Acc@1  31.25 (  8.56)	Acc@5  37.50 ( 37.54)
Epoch: [0][320/812]	Time  0.338 ( 0.514)	Data  0.000 ( 0.012)	Loss 3.0837640762329102 (3.0952580851557840)	Acc@1  12.50 (  8.63)	Acc@5  37.50 ( 37.79)
Epoch: [0][330/812]	Time  0.495 ( 0.510)	Data  0.000 ( 0.011)	Loss 2.8884577751159668 (3.0917392266841093)	Acc@1   0.00 (  8.63)	Acc@5  37.50 ( 37.90)
Epoch: [0][340/812]	Time  0.821 ( 0.514)	Data  0.001 ( 0.011)	Loss 2.8182344436645508 (3.0899216674290093)	Acc@1   0.00 (  8.54)	Acc@5  50.00 ( 37.99)
Epoch: [0][350/812]	Time  0.474 ( 0.520)	Data  0.000 ( 0.011)	Loss 3.1426131725311279 (3.0906730941218190)	Acc@1  12.50 (  8.62)	Acc@5  37.50 ( 38.11)
Epoch: [0][360/812]	Time  0.247 ( 0.514)	Data  0.000 ( 0.010)	Loss 3.3756151199340820 (3.0868458424248524)	Acc@1  12.50 (  8.73)	Acc@5  25.00 ( 38.21)
Epoch: [0][370/812]	Time  0.426 ( 0.508)	Data  0.000 ( 0.010)	Loss 3.0018181800842285 (3.0839322660811184)	Acc@1   6.25 (  8.69)	Acc@5  31.25 ( 38.24)
Epoch: [0][380/812]	Time  0.278 ( 0.506)	Data  0.000 ( 0.010)	Loss 2.9005923271179199 (3.0808879546918893)	Acc@1   0.00 (  8.69)	Acc@5  37.50 ( 38.37)
Epoch: [0][390/812]	Time  0.310 ( 0.502)	Data  0.000 ( 0.010)	Loss 2.8232371807098389 (3.0771197638548244)	Acc@1  25.00 (  8.81)	Acc@5  62.50 ( 38.43)
Epoch: [0][400/812]	Time  0.283 ( 0.502)	Data  0.000 ( 0.009)	Loss 2.7842588424682617 (3.0720907595389502)	Acc@1  18.75 (  8.87)	Acc@5  37.50 ( 38.72)
Epoch: [0][410/812]	Time  0.753 ( 0.505)	Data  0.001 ( 0.009)	Loss 2.9428956508636475 (3.0746814363484254)	Acc@1  12.50 (  8.82)	Acc@5  56.25 ( 38.72)
Epoch: [0][420/812]	Time  0.399 ( 0.502)	Data  0.000 ( 0.009)	Loss 2.8044629096984863 (3.0728471873774947)	Acc@1   6.25 (  8.79)	Acc@5  43.75 ( 38.72)
Epoch: [0][430/812]	Time  0.378 ( 0.505)	Data  0.000 ( 0.009)	Loss 3.0596561431884766 (3.0712291845865978)	Acc@1   0.00 (  8.77)	Acc@5  37.50 ( 38.67)
Epoch: [0][440/812]	Time  0.839 ( 0.510)	Data  0.001 ( 0.009)	Loss 2.9096627235412598 (3.0675303752189862)	Acc@1   6.25 (  8.83)	Acc@5  37.50 ( 38.80)
Epoch: [0][450/812]	Time  0.280 ( 0.507)	Data  0.000 ( 0.008)	Loss 3.2193555831909180 (3.0627840622565699)	Acc@1   6.25 (  8.91)	Acc@5  31.25 ( 38.87)
Epoch: [0][460/812]	Time  0.279 ( 0.503)	Data  0.000 ( 0.008)	Loss 2.9812231063842773 (3.0584867947013672)	Acc@1   6.25 (  8.96)	Acc@5  18.75 ( 39.00)
Epoch: [0][470/812]	Time  0.799 ( 0.506)	Data  0.001 ( 0.008)	Loss 2.6163344383239746 (3.0541952759850051)	Acc@1  31.25 (  9.02)	Acc@5  62.50 ( 39.20)
Epoch: [0][480/812]	Time  0.261 ( 0.501)	Data  0.000 ( 0.008)	Loss 3.1702730655670166 (3.0513447312456159)	Acc@1  25.00 (  9.15)	Acc@5  31.25 ( 39.36)
Epoch: [0][490/812]	Time  0.400 ( 0.498)	Data  0.000 ( 0.008)	Loss 3.0569634437561035 (3.0500494783133449)	Acc@1  18.75 (  9.18)	Acc@5  37.50 ( 39.40)
Epoch: [0][500/812]	Time  0.404 ( 0.498)	Data  0.000 ( 0.008)	Loss 3.0764989852905273 (3.0475914582996788)	Acc@1  12.50 (  9.23)	Acc@5  37.50 ( 39.45)
Epoch: [0][510/812]	Time  0.339 ( 0.502)	Data  0.000 ( 0.007)	Loss 3.4034857749938965 (3.0441175668906792)	Acc@1   0.00 (  9.27)	Acc@5  25.00 ( 39.46)
Epoch: [0][520/812]	Time  0.432 ( 0.499)	Data  0.000 ( 0.007)	Loss 3.3219199180603027 (3.0451374973765719)	Acc@1   6.25 (  9.21)	Acc@5  31.25 ( 39.43)
Epoch: [0][530/812]	Time  0.705 ( 0.504)	Data  0.003 ( 0.007)	Loss 3.4785332679748535 (3.0445308752652616)	Acc@1   6.25 (  9.17)	Acc@5  31.25 ( 39.48)
Epoch: [0][540/812]	Time  0.431 ( 0.504)	Data  0.000 ( 0.007)	Loss 2.9882056713104248 (3.0438126534939696)	Acc@1  18.75 (  9.17)	Acc@5  37.50 ( 39.51)
Epoch: [0][550/812]	Time  0.379 ( 0.507)	Data  0.000 ( 0.007)	Loss 2.8681392669677734 (3.0409217819760803)	Acc@1   6.25 (  9.17)	Acc@5  31.25 ( 39.54)
Epoch: [0][560/812]	Time  0.310 ( 0.507)	Data  0.000 ( 0.007)	Loss 2.9166510105133057 (3.0370036777965526)	Acc@1  12.50 (  9.27)	Acc@5  37.50 ( 39.72)
Epoch: [0][570/812]	Time  0.734 ( 0.506)	Data  0.000 ( 0.007)	Loss 2.4344327449798584 (3.0343407173290351)	Acc@1  25.00 (  9.39)	Acc@5  75.00 ( 39.85)
Epoch: [0][580/812]	Time  0.409 ( 0.506)	Data  0.000 ( 0.007)	Loss 2.9390428066253662 (3.0334061078483594)	Acc@1  12.50 (  9.47)	Acc@5  37.50 ( 39.85)
Epoch: [0][590/812]	Time  0.884 ( 0.506)	Data  0.000 ( 0.007)	Loss 2.7532103061676025 (3.0302522368035745)	Acc@1   6.25 (  9.55)	Acc@5  50.00 ( 39.97)
Epoch: [0][600/812]	Time  0.820 ( 0.509)	Data  0.000 ( 0.006)	Loss 2.5478227138519287 (3.0284056572271463)	Acc@1  25.00 (  9.62)	Acc@5  68.75 ( 40.10)
Epoch: [0][610/812]	Time  0.353 ( 0.507)	Data  0.000 ( 0.006)	Loss 3.1294014453887939 (3.0302035359821224)	Acc@1   6.25 (  9.63)	Acc@5  25.00 ( 40.03)
Epoch: [0][620/812]	Time  0.436 ( 0.504)	Data  0.000 ( 0.006)	Loss 2.9510836601257324 (3.0289813758094528)	Acc@1   6.25 (  9.61)	Acc@5  50.00 ( 39.98)
Epoch: [0][630/812]	Time  0.420 ( 0.504)	Data  0.000 ( 0.006)	Loss 3.2533440589904785 (3.0288645658175652)	Acc@1   6.25 (  9.55)	Acc@5  12.50 ( 39.83)
Epoch: [0][640/812]	Time  0.414 ( 0.508)	Data  0.000 ( 0.006)	Loss 2.8177978992462158 (3.0282292949985976)	Acc@1   0.00 (  9.49)	Acc@5  50.00 ( 39.81)
Epoch: [0][650/812]	Time  0.527 ( 0.506)	Data  0.000 ( 0.006)	Loss 2.7859268188476562 (3.0259533781792896)	Acc@1  12.50 (  9.48)	Acc@5  50.00 ( 39.88)
Epoch: [0][660/812]	Time  0.525 ( 0.503)	Data  0.000 ( 0.006)	Loss 2.8977997303009033 (3.0257024115765869)	Acc@1   6.25 (  9.49)	Acc@5  37.50 ( 39.87)
Epoch: [0][670/812]	Time  0.333 ( 0.504)	Data  0.000 ( 0.006)	Loss 3.0291996002197266 (3.0259508574950535)	Acc@1   6.25 (  9.49)	Acc@5  37.50 ( 39.83)
Epoch: [0][680/812]	Time  0.451 ( 0.503)	Data  0.001 ( 0.006)	Loss 2.7975561618804932 (3.0254012470553451)	Acc@1  18.75 (  9.50)	Acc@5  37.50 ( 39.77)
Epoch: [0][690/812]	Time  0.268 ( 0.505)	Data  0.000 ( 0.006)	Loss 2.8087997436523438 (3.0253102775938086)	Acc@1  25.00 (  9.58)	Acc@5  37.50 ( 39.76)
Epoch: [0][700/812]	Time  0.270 ( 0.502)	Data  0.000 ( 0.006)	Loss 2.9902925491333008 (3.0246806739910523)	Acc@1   6.25 (  9.62)	Acc@5  50.00 ( 39.88)
Epoch: [0][710/812]	Time  0.390 ( 0.500)	Data  0.000 ( 0.006)	Loss 2.9195744991302490 (3.0249906337378567)	Acc@1   6.25 (  9.59)	Acc@5  43.75 ( 39.83)
Epoch: [0][720/812]	Time  0.350 ( 0.506)	Data  0.000 ( 0.005)	Loss 2.7398607730865479 (3.0235440972443262)	Acc@1  12.50 (  9.63)	Acc@5  50.00 ( 39.97)
Epoch: [0][730/812]	Time  0.312 ( 0.504)	Data  0.000 ( 0.005)	Loss 3.0007987022399902 (3.0238791643839367)	Acc@1   0.00 (  9.65)	Acc@5  31.25 ( 39.95)
Epoch: [0][740/812]	Time  0.296 ( 0.501)	Data  0.000 ( 0.005)	Loss 3.2429418563842773 (3.0224669204871502)	Acc@1   0.00 (  9.66)	Acc@5  31.25 ( 39.95)
Epoch: [0][750/812]	Time  0.389 ( 0.499)	Data  0.000 ( 0.005)	Loss 2.9452543258666992 (3.0223632681068189)	Acc@1   0.00 (  9.65)	Acc@5  37.50 ( 39.93)
Epoch: [0][760/812]	Time  0.307 ( 0.497)	Data  0.000 ( 0.005)	Loss 2.9727699756622314 (3.0215390555648702)	Acc@1  12.50 (  9.64)	Acc@5  37.50 ( 39.93)
Epoch: [0][770/812]	Time  0.303 ( 0.494)	Data  0.000 ( 0.005)	Loss 2.9044034481048584 (3.0210435959460979)	Acc@1  12.50 (  9.58)	Acc@5  43.75 ( 39.92)
Epoch: [0][780/812]	Time  0.452 ( 0.494)	Data  0.000 ( 0.005)	Loss 2.9230077266693115 (3.0196392994073533)	Acc@1   6.25 (  9.64)	Acc@5  43.75 ( 39.92)
Epoch: [0][790/812]	Time  0.359 ( 0.495)	Data  0.000 ( 0.005)	Loss 2.8523428440093994 (3.0187973038737299)	Acc@1   0.00 (  9.61)	Acc@5  50.00 ( 39.89)
Epoch: [0][800/812]	Time  0.370 ( 0.494)	Data  0.000 ( 0.005)	Loss 2.9898011684417725 (3.0183829296840710)	Acc@1   6.25 (  9.60)	Acc@5  37.50 ( 39.89)
Epoch: [0][810/812]	Time  0.291 ( 0.496)	Data  0.000 ( 0.005)	Loss 2.9275226593017578 (3.0180428683684286)	Acc@1  12.50 (  9.63)	Acc@5  37.50 ( 39.93)
epoch: 0, Avg_Loss 3.018083704222599
Test: [  0/204]	Time  2.886 ( 2.886)	Loss 2.9317e+00 (2.9317e+00)	Acc@1  12.50 ( 12.50)	Acc@5  43.75 ( 43.75)
Test: [ 10/204]	Time  0.072 ( 0.341)	Loss 3.8338e+00 (3.3777e+00)	Acc@1   6.25 (  8.52)	Acc@5  43.75 ( 41.48)
Test: [ 20/204]	Time  0.119 ( 0.252)	Loss 3.1575e+00 (3.3199e+00)	Acc@1   0.00 (  6.25)	Acc@5  18.75 ( 40.77)
Test: [ 30/204]	Time  0.149 ( 0.228)	Loss 3.4473e+00 (3.2564e+00)	Acc@1   0.00 (  6.45)	Acc@5  18.75 ( 39.11)
Test: [ 40/204]	Time  0.095 ( 0.205)	Loss 3.4984e+00 (3.2570e+00)	Acc@1  12.50 (  7.77)	Acc@5  18.75 ( 38.72)
Test: [ 50/204]	Time  0.064 ( 0.177)	Loss 3.3362e+00 (3.2786e+00)	Acc@1   6.25 (  7.84)	Acc@5  31.25 ( 37.87)
Test: [ 60/204]	Time  0.101 ( 0.162)	Loss 3.7383e+00 (3.2989e+00)	Acc@1   6.25 (  7.68)	Acc@5  31.25 ( 36.37)
Test: [ 70/204]	Time  0.162 ( 0.154)	Loss 2.8896e+00 (3.2864e+00)	Acc@1   6.25 (  7.83)	Acc@5  43.75 ( 36.80)
Test: [ 80/204]	Time  0.132 ( 0.150)	Loss 3.1053e+00 (3.2850e+00)	Acc@1  12.50 (  8.26)	Acc@5  43.75 ( 36.27)
Test: [ 90/204]	Time  0.134 ( 0.143)	Loss 3.3105e+00 (3.2770e+00)	Acc@1   6.25 (  8.65)	Acc@5  50.00 ( 36.54)
Test: [100/204]	Time  0.259 ( 0.145)	Loss 3.0140e+00 (3.2759e+00)	Acc@1  12.50 (  8.66)	Acc@5  37.50 ( 36.82)
Test: [110/204]	Time  0.143 ( 0.147)	Loss 3.0449e+00 (3.2801e+00)	Acc@1  12.50 (  8.78)	Acc@5  25.00 ( 36.66)
Test: [120/204]	Time  0.196 ( 0.147)	Loss 4.1317e+00 (3.2768e+00)	Acc@1   0.00 (  8.78)	Acc@5  18.75 ( 36.42)
Test: [130/204]	Time  0.110 ( 0.149)	Loss 3.8666e+00 (3.2712e+00)	Acc@1  25.00 (  8.92)	Acc@5  37.50 ( 36.59)
Test: [140/204]	Time  0.083 ( 0.161)	Loss 3.6966e+00 (3.2798e+00)	Acc@1  18.75 (  8.95)	Acc@5  43.75 ( 36.70)
Test: [150/204]	Time  0.115 ( 0.156)	Loss 2.8448e+00 (3.2866e+00)	Acc@1  12.50 (  8.90)	Acc@5  31.25 ( 36.67)
Test: [160/204]	Time  0.114 ( 0.155)	Loss 3.5146e+00 (3.2802e+00)	Acc@1   0.00 (  8.70)	Acc@5  31.25 ( 36.34)
Test: [170/204]	Time  0.133 ( 0.154)	Loss 3.3510e+00 (3.2760e+00)	Acc@1  12.50 (  8.85)	Acc@5  43.75 ( 36.44)
Test: [180/204]	Time  0.193 ( 0.153)	Loss 3.1378e+00 (3.2708e+00)	Acc@1   6.25 (  8.84)	Acc@5  25.00 ( 36.46)
Test: [190/204]	Time  0.095 ( 0.149)	Loss 3.0834e+00 (3.2690e+00)	Acc@1   0.00 (  8.70)	Acc@5  12.50 ( 36.45)
Test: [200/204]	Time  0.120 ( 0.146)	Loss 3.2435e+00 (3.2652e+00)	Acc@1  12.50 (  8.96)	Acc@5  31.25 ( 36.32)
 * Acc@1 8.848 Acc@5 36.160
Epoch: [1][  0/812]	Time  5.216 ( 5.216)	Data  4.676 ( 4.676)	Loss 3.1121277809143066 (3.1121277809143066)	Acc@1   0.00 (  0.00)	Acc@5  50.00 ( 50.00)
Epoch: [1][ 10/812]	Time  0.795 ( 0.988)	Data  0.000 ( 0.426)	Loss 3.1164076328277588 (3.0858756845647637)	Acc@1   6.25 (  5.11)	Acc@5  25.00 ( 31.25)
Epoch: [1][ 20/812]	Time  0.344 ( 0.807)	Data  0.000 ( 0.223)	Loss 3.0496351718902588 (3.0693807261330739)	Acc@1  18.75 (  6.85)	Acc@5  31.25 ( 32.14)
Epoch: [1][ 30/812]	Time  0.365 ( 0.648)	Data  0.000 ( 0.151)	Loss 2.9667053222656250 (3.0126366076930875)	Acc@1  12.50 (  7.26)	Acc@5  56.25 ( 35.89)
Epoch: [1][ 40/812]	Time  0.299 ( 0.567)	Data  0.000 ( 0.115)	Loss 3.0441129207611084 (3.0091605302764148)	Acc@1  12.50 (  8.23)	Acc@5  31.25 ( 35.82)
Epoch: [1][ 50/812]	Time  0.304 ( 0.516)	Data  0.000 ( 0.092)	Loss 2.7066204547882080 (3.0035527547200522)	Acc@1  12.50 (  7.84)	Acc@5  56.25 ( 35.91)
Epoch: [1][ 60/812]	Time  0.509 ( 0.545)	Data  0.000 ( 0.077)	Loss 3.0052027702331543 (3.0055743123664231)	Acc@1   6.25 (  8.20)	Acc@5  50.00 ( 36.68)
Epoch: [1][ 70/812]	Time  0.443 ( 0.521)	Data  0.000 ( 0.066)	Loss 3.1195065975189209 (3.0083796104914704)	Acc@1   0.00 (  8.19)	Acc@5  25.00 ( 36.09)
Epoch: [1][ 80/812]	Time  0.365 ( 0.502)	Data  0.000 ( 0.058)	Loss 2.8034560680389404 (2.9999856006951982)	Acc@1  12.50 (  7.79)	Acc@5  31.25 ( 36.34)
Epoch: [1][ 90/812]	Time  0.345 ( 0.496)	Data  0.000 ( 0.052)	Loss 2.9779064655303955 (2.9974579496698066)	Acc@1   0.00 (  8.24)	Acc@5  50.00 ( 37.09)
Epoch: [1][100/812]	Time  0.290 ( 0.474)	Data  0.000 ( 0.047)	Loss 2.9307575225830078 (2.9954014485425287)	Acc@1  25.00 (  8.91)	Acc@5  56.25 ( 37.69)
Epoch: [1][110/812]	Time  0.359 ( 0.478)	Data  0.000 ( 0.043)	Loss 3.1878201961517334 (2.9984467609508618)	Acc@1  12.50 (  9.01)	Acc@5  43.75 ( 37.67)
Epoch: [1][120/812]	Time  0.404 ( 0.491)	Data  0.000 ( 0.039)	Loss 2.9818654060363770 (2.9886837360287499)	Acc@1   6.25 (  9.40)	Acc@5  50.00 ( 38.33)
Epoch: [1][130/812]	Time  0.283 ( 0.487)	Data  0.000 ( 0.036)	Loss 2.8477749824523926 (2.9925474064950723)	Acc@1  18.75 (  9.11)	Acc@5  37.50 ( 38.26)
Epoch: [1][140/812]	Time  0.328 ( 0.475)	Data  0.000 ( 0.034)	Loss 3.1090102195739746 (2.9934696728456105)	Acc@1   6.25 (  9.04)	Acc@5  25.00 ( 38.08)
Epoch: [1][150/812]	Time  0.377 ( 0.465)	Data  0.000 ( 0.031)	Loss 3.0984396934509277 (3.0006205972456774)	Acc@1   6.25 (  9.06)	Acc@5  31.25 ( 38.08)
Epoch: [1][160/812]	Time  0.299 ( 0.455)	Data  0.000 ( 0.030)	Loss 2.8812589645385742 (3.0038584552196244)	Acc@1   6.25 (  8.81)	Acc@5  43.75 ( 37.77)
Epoch: [1][170/812]	Time  0.289 ( 0.451)	Data  0.000 ( 0.028)	Loss 2.7928764820098877 (3.0006594072308457)	Acc@1  25.00 (  8.92)	Acc@5  43.75 ( 37.72)
Epoch: [1][180/812]	Time  1.427 ( 0.453)	Data  0.001 ( 0.026)	Loss 3.0032956600189209 (2.9941931284593615)	Acc@1  12.50 (  8.91)	Acc@5  31.25 ( 37.67)
Epoch: [1][190/812]	Time  0.307 ( 0.444)	Data  0.000 ( 0.025)	Loss 2.9948306083679199 (2.9950536096283278)	Acc@1   6.25 (  9.00)	Acc@5  31.25 ( 37.50)
Epoch: [1][200/812]	Time  0.341 ( 0.439)	Data  0.000 ( 0.024)	Loss 2.7620024681091309 (2.9941854963255166)	Acc@1  25.00 (  9.02)	Acc@5  62.50 ( 37.53)
Epoch: [1][210/812]	Time  0.296 ( 0.432)	Data  0.000 ( 0.023)	Loss 3.1227042675018311 (2.9952268882950337)	Acc@1  12.50 (  8.95)	Acc@5  25.00 ( 37.47)
Epoch: [1][220/812]	Time  0.425 ( 0.432)	Data  0.000 ( 0.022)	Loss 2.6908307075500488 (2.9904586761785308)	Acc@1  18.75 (  8.88)	Acc@5  43.75 ( 37.50)
Epoch: [1][230/812]	Time  0.428 ( 0.441)	Data  0.001 ( 0.021)	Loss 3.0511672496795654 (2.9910679121554158)	Acc@1   0.00 (  8.82)	Acc@5  25.00 ( 37.53)
Epoch: [1][240/812]	Time  0.299 ( 0.446)	Data  0.000 ( 0.020)	Loss 2.9293282032012939 (2.9908294895377892)	Acc@1   6.25 (  8.79)	Acc@5  50.00 ( 37.34)
Epoch: [1][250/812]	Time  0.323 ( 0.445)	Data  0.000 ( 0.019)	Loss 3.3180413246154785 (2.9902508496288283)	Acc@1   0.00 (  8.69)	Acc@5  31.25 ( 37.50)
Epoch: [1][260/812]	Time  0.303 ( 0.444)	Data  0.000 ( 0.018)	Loss 2.8968493938446045 (2.9879340175467890)	Acc@1   6.25 (  8.57)	Acc@5  50.00 ( 37.52)
Epoch: [1][270/812]	Time  1.428 ( 0.444)	Data  0.000 ( 0.018)	Loss 3.2377231121063232 (2.9906529544464338)	Acc@1   0.00 (  8.44)	Acc@5  25.00 ( 37.36)
Epoch: [1][280/812]	Time  0.303 ( 0.440)	Data  0.000 ( 0.017)	Loss 3.0708296298980713 (2.9914156092443500)	Acc@1  12.50 (  8.43)	Acc@5  37.50 ( 37.03)
Epoch: [1][290/812]	Time  0.311 ( 0.436)	Data  0.000 ( 0.017)	Loss 2.8510966300964355 (2.9897029326134121)	Acc@1   6.25 (  8.53)	Acc@5  56.25 ( 37.18)
Epoch: [1][300/812]	Time  0.481 ( 0.434)	Data  0.000 ( 0.016)	Loss 2.8486831188201904 (2.9876893762734245)	Acc@1   6.25 (  8.62)	Acc@5  18.75 ( 37.13)
Epoch: [1][310/812]	Time  0.546 ( 0.447)	Data  0.000 ( 0.015)	Loss 2.7727253437042236 (2.9850346129806864)	Acc@1   6.25 (  8.62)	Acc@5  37.50 ( 37.22)
Epoch: [1][320/812]	Time  1.186 ( 0.451)	Data  0.015 ( 0.015)	Loss 2.7908444404602051 (2.9813588965347622)	Acc@1  12.50 (  8.74)	Acc@5  43.75 ( 37.44)
Epoch: [1][330/812]	Time  0.549 ( 0.451)	Data  0.000 ( 0.015)	Loss 2.8466227054595947 (2.9786776077351180)	Acc@1   0.00 (  8.76)	Acc@5  43.75 ( 37.56)
Epoch: [1][340/812]	Time  1.166 ( 0.453)	Data  0.000 ( 0.014)	Loss 3.0564169883728027 (2.9763132519036795)	Acc@1   6.25 (  8.76)	Acc@5  31.25 ( 37.76)
Epoch: [1][350/812]	Time  0.382 ( 0.453)	Data  0.000 ( 0.014)	Loss 2.8136284351348877 (2.9712277260261382)	Acc@1  12.50 (  8.81)	Acc@5  50.00 ( 38.02)
Epoch: [1][360/812]	Time  1.020 ( 0.457)	Data  0.000 ( 0.013)	Loss 3.0015332698822021 (2.9715178475155395)	Acc@1   6.25 (  8.86)	Acc@5  43.75 ( 38.07)
Epoch: [1][370/812]	Time  0.346 ( 0.455)	Data  0.000 ( 0.013)	Loss 2.8230128288269043 (2.9674303056094846)	Acc@1   6.25 (  8.98)	Acc@5  56.25 ( 38.38)
Epoch: [1][380/812]	Time  0.336 ( 0.452)	Data  0.000 ( 0.013)	Loss 2.8744356632232666 (2.9670857212987785)	Acc@1  12.50 (  9.09)	Acc@5  56.25 ( 38.30)
Epoch: [1][390/812]	Time  0.401 ( 0.450)	Data  0.000 ( 0.012)	Loss 2.9113779067993164 (2.9656047955193481)	Acc@1   0.00 (  9.10)	Acc@5  37.50 ( 38.33)
Epoch: [1][400/812]	Time  0.504 ( 0.449)	Data  0.000 ( 0.012)	Loss 3.2372987270355225 (2.9628330109422643)	Acc@1   6.25 (  9.23)	Acc@5  37.50 ( 38.58)
Epoch: [1][410/812]	Time  0.290 ( 0.445)	Data  0.000 ( 0.012)	Loss 3.0407681465148926 (2.9652904002335818)	Acc@1   6.25 (  9.18)	Acc@5  43.75 ( 38.43)
Epoch: [1][420/812]	Time  0.263 ( 0.441)	Data  0.000 ( 0.012)	Loss 2.9083390235900879 (2.9654800982486607)	Acc@1  18.75 (  9.22)	Acc@5  50.00 ( 38.44)
Epoch: [1][430/812]	Time  0.810 ( 0.440)	Data  0.000 ( 0.012)	Loss 2.8222780227661133 (2.9636431442889148)	Acc@1   6.25 (  9.24)	Acc@5  43.75 ( 38.52)
Epoch: [1][440/812]	Time  0.468 ( 0.441)	Data  0.000 ( 0.012)	Loss 2.9928147792816162 (2.9624009262136863)	Acc@1   6.25 (  9.25)	Acc@5  43.75 ( 38.62)
Epoch: [1][450/812]	Time  0.315 ( 0.440)	Data  0.000 ( 0.012)	Loss 2.8326189517974854 (2.9596758094965221)	Acc@1   6.25 (  9.34)	Acc@5  37.50 ( 38.83)
Epoch: [1][460/812]	Time  0.293 ( 0.437)	Data  0.000 ( 0.012)	Loss 2.9792497158050537 (2.9605282821779397)	Acc@1   6.25 (  9.37)	Acc@5  37.50 ( 38.83)
Epoch: [1][470/812]	Time  0.305 ( 0.439)	Data  0.000 ( 0.012)	Loss 3.4269413948059082 (2.9619112991729866)	Acc@1   6.25 (  9.26)	Acc@5  12.50 ( 38.76)
Epoch: [1][480/812]	Time  0.610 ( 0.437)	Data  0.000 ( 0.011)	Loss 3.0864260196685791 (2.9638754131888154)	Acc@1   0.00 (  9.13)	Acc@5  18.75 ( 38.50)
Epoch: [1][490/812]	Time  0.390 ( 0.435)	Data  0.000 ( 0.011)	Loss 2.9274525642395020 (2.9681677201614844)	Acc@1   6.25 (  9.06)	Acc@5  31.25 ( 38.19)
Epoch: [1][500/812]	Time  0.346 ( 0.437)	Data  0.000 ( 0.011)	Loss 3.1049311161041260 (2.9693769271264294)	Acc@1   0.00 (  9.01)	Acc@5  31.25 ( 38.07)
Epoch: [1][510/812]	Time  0.279 ( 0.438)	Data  0.000 ( 0.011)	Loss 3.2333819866180420 (2.9710360767790016)	Acc@1   0.00 (  8.97)	Acc@5   6.25 ( 38.03)
Epoch: [1][520/812]	Time  0.667 ( 0.437)	Data  0.000 ( 0.011)	Loss 3.1595201492309570 (2.9716992469758310)	Acc@1   6.25 (  8.95)	Acc@5  25.00 ( 38.03)
Epoch: [1][530/812]	Time  0.263 ( 0.435)	Data  0.000 ( 0.010)	Loss 3.0537447929382324 (2.9730123149024115)	Acc@1   0.00 (  8.91)	Acc@5  37.50 ( 37.89)
Epoch: [1][540/812]	Time  0.570 ( 0.436)	Data  0.000 ( 0.010)	Loss 2.9928765296936035 (2.9751171761652015)	Acc@1   0.00 (  8.90)	Acc@5  25.00 ( 37.81)
Epoch: [1][550/812]	Time  0.286 ( 0.437)	Data  0.000 ( 0.010)	Loss 2.9775180816650391 (2.9761436837987327)	Acc@1   6.25 (  8.90)	Acc@5  37.50 ( 37.83)
Epoch: [1][560/812]	Time  0.310 ( 0.434)	Data  0.000 ( 0.010)	Loss 2.7095224857330322 (2.9744488427983247)	Acc@1  18.75 (  9.00)	Acc@5  43.75 ( 37.89)
Epoch: [1][570/812]	Time  0.301 ( 0.432)	Data  0.000 ( 0.010)	Loss 2.9288105964660645 (2.9750937776682465)	Acc@1  12.50 (  9.02)	Acc@5  37.50 ( 37.90)
Epoch: [1][580/812]	Time  0.358 ( 0.430)	Data  0.000 ( 0.009)	Loss 3.0085659027099609 (2.9752292501741762)	Acc@1   6.25 (  9.06)	Acc@5  18.75 ( 37.79)
Epoch: [1][590/812]	Time  0.283 ( 0.428)	Data  0.000 ( 0.009)	Loss 3.1582083702087402 (2.9779170676939781)	Acc@1  12.50 (  9.02)	Acc@5  18.75 ( 37.62)
Epoch: [1][600/812]	Time  0.362 ( 0.427)	Data  0.000 ( 0.009)	Loss 3.0904245376586914 (2.9794134506568337)	Acc@1   0.00 (  9.03)	Acc@5  18.75 ( 37.48)
Epoch: [1][610/812]	Time  0.284 ( 0.425)	Data  0.000 ( 0.009)	Loss 3.1696133613586426 (2.9809065169476447)	Acc@1   6.25 (  8.98)	Acc@5   6.25 ( 37.23)
Epoch: [1][620/812]	Time  0.333 ( 0.423)	Data  0.000 ( 0.009)	Loss 2.9855456352233887 (2.9817760209530446)	Acc@1   6.25 (  8.96)	Acc@5  31.25 ( 37.14)
Epoch: [1][630/812]	Time  0.289 ( 0.422)	Data  0.000 ( 0.009)	Loss 3.1803295612335205 (2.9825356826540408)	Acc@1   0.00 (  8.92)	Acc@5  12.50 ( 37.03)
Epoch: [1][640/812]	Time  0.313 ( 0.420)	Data  0.000 ( 0.009)	Loss 3.2397336959838867 (2.9836325842579146)	Acc@1   6.25 (  8.89)	Acc@5  31.25 ( 36.94)
Epoch: [1][650/812]	Time  0.520 ( 0.422)	Data  0.000 ( 0.009)	Loss 3.0266969203948975 (2.9839844249543690)	Acc@1   0.00 (  8.86)	Acc@5  37.50 ( 36.90)
Epoch: [1][660/812]	Time  0.273 ( 0.424)	Data  0.000 ( 0.008)	Loss 3.0029456615447998 (2.9845107975958336)	Acc@1   6.25 (  8.80)	Acc@5  25.00 ( 36.85)
Epoch: [1][670/812]	Time  0.312 ( 0.422)	Data  0.000 ( 0.008)	Loss 3.0255713462829590 (2.9848348568519785)	Acc@1   6.25 (  8.74)	Acc@5  18.75 ( 36.80)
Epoch: [1][680/812]	Time  0.301 ( 0.422)	Data  0.000 ( 0.008)	Loss 3.0773494243621826 (2.9858138015091682)	Acc@1   6.25 (  8.69)	Acc@5  43.75 ( 36.80)
Epoch: [1][690/812]	Time  0.276 ( 0.420)	Data  0.000 ( 0.008)	Loss 2.9164717197418213 (2.9867584401722755)	Acc@1   6.25 (  8.66)	Acc@5  37.50 ( 36.71)
Epoch: [1][700/812]	Time  0.322 ( 0.421)	Data  0.000 ( 0.008)	Loss 2.8034098148345947 (2.9856180998466155)	Acc@1  12.50 (  8.68)	Acc@5  62.50 ( 36.80)
Epoch: [1][710/812]	Time  0.306 ( 0.420)	Data  0.000 ( 0.008)	Loss 2.7487952709197998 (2.9850747132603126)	Acc@1  31.25 (  8.76)	Acc@5  50.00 ( 36.87)
Epoch: [1][720/812]	Time  0.319 ( 0.420)	Data  0.000 ( 0.008)	Loss 3.0185627937316895 (2.9846433796267569)	Acc@1   6.25 (  8.72)	Acc@5  50.00 ( 36.88)
Epoch: [1][730/812]	Time  0.517 ( 0.420)	Data  0.000 ( 0.008)	Loss 2.9696903228759766 (2.9844136564252151)	Acc@1   6.25 (  8.68)	Acc@5  31.25 ( 36.84)
Epoch: [1][740/812]	Time  0.335 ( 0.419)	Data  0.000 ( 0.008)	Loss 2.7998504638671875 (2.9834212899690700)	Acc@1   0.00 (  8.69)	Acc@5  50.00 ( 36.87)
Epoch: [1][750/812]	Time  0.366 ( 0.418)	Data  0.000 ( 0.007)	Loss 2.9816370010375977 (2.9822375060080213)	Acc@1   0.00 (  8.69)	Acc@5  25.00 ( 36.92)
Epoch: [1][760/812]	Time  0.433 ( 0.417)	Data  0.000 ( 0.007)	Loss 2.9770309925079346 (2.9832091397274181)	Acc@1   6.25 (  8.68)	Acc@5  43.75 ( 36.86)
Epoch: [1][770/812]	Time  0.378 ( 0.418)	Data  0.009 ( 0.007)	Loss 3.0850822925567627 (2.9842516494632232)	Acc@1   0.00 (  8.65)	Acc@5  31.25 ( 36.75)
Epoch: [1][780/812]	Time  0.329 ( 0.416)	Data  0.000 ( 0.007)	Loss 3.2743287086486816 (2.9839308667885049)	Acc@1  12.50 (  8.62)	Acc@5  31.25 ( 36.80)
Epoch: [1][790/812]	Time  0.357 ( 0.418)	Data  0.000 ( 0.007)	Loss 2.9666104316711426 (2.9835380314273570)	Acc@1   0.00 (  8.64)	Acc@5  37.50 ( 36.82)
Epoch: [1][800/812]	Time  0.302 ( 0.418)	Data  0.000 ( 0.007)	Loss 2.7845623493194580 (2.9827919348646490)	Acc@1  18.75 (  8.65)	Acc@5  25.00 ( 36.86)
Epoch: [1][810/812]	Time  0.350 ( 0.418)	Data  0.000 ( 0.007)	Loss 2.7923157215118408 (2.9823015444669712)	Acc@1  12.50 (  8.68)	Acc@5  62.50 ( 36.92)
epoch: 1, Avg_Loss 2.9831616062248867
Test: [  0/204]	Time  2.841 ( 2.841)	Loss 2.9694e+00 (2.9694e+00)	Acc@1  12.50 ( 12.50)	Acc@5  43.75 ( 43.75)
Test: [ 10/204]	Time  0.135 ( 0.372)	Loss 2.9924e+00 (2.9545e+00)	Acc@1   0.00 (  5.11)	Acc@5  31.25 ( 38.64)
Test: [ 20/204]	Time  0.144 ( 0.236)	Loss 3.0327e+00 (3.0034e+00)	Acc@1   6.25 (  5.65)	Acc@5  37.50 ( 36.31)
Test: [ 30/204]	Time  0.066 ( 0.203)	Loss 2.8684e+00 (2.9845e+00)	Acc@1  12.50 (  6.05)	Acc@5  50.00 ( 37.70)
Test: [ 40/204]	Time  0.113 ( 0.171)	Loss 3.0612e+00 (2.9992e+00)	Acc@1   0.00 (  5.49)	Acc@5  31.25 ( 36.74)
Test: [ 50/204]	Time  0.104 ( 0.156)	Loss 3.0334e+00 (3.0221e+00)	Acc@1  18.75 (  5.39)	Acc@5  37.50 ( 34.68)
Test: [ 60/204]	Time  0.111 ( 0.144)	Loss 2.9069e+00 (3.0460e+00)	Acc@1  12.50 (  5.53)	Acc@5  62.50 ( 33.91)
Test: [ 70/204]	Time  0.270 ( 0.142)	Loss 3.2159e+00 (3.0530e+00)	Acc@1   0.00 (  5.37)	Acc@5  25.00 ( 34.51)
Test: [ 80/204]	Time  0.101 ( 0.143)	Loss 3.0385e+00 (3.0551e+00)	Acc@1   0.00 (  5.32)	Acc@5  37.50 ( 34.10)
Test: [ 90/204]	Time  0.139 ( 0.143)	Loss 2.9605e+00 (3.0597e+00)	Acc@1   0.00 (  5.43)	Acc@5  31.25 ( 33.10)
Test: [100/204]	Time  0.088 ( 0.138)	Loss 3.1073e+00 (3.0619e+00)	Acc@1   0.00 (  5.63)	Acc@5   6.25 ( 32.92)
Test: [110/204]	Time  0.261 ( 0.148)	Loss 2.8955e+00 (3.0588e+00)	Acc@1  12.50 (  5.74)	Acc@5  37.50 ( 33.05)
Test: [120/204]	Time  0.274 ( 0.148)	Loss 2.9728e+00 (3.0538e+00)	Acc@1   0.00 (  5.89)	Acc@5  25.00 ( 33.32)
Test: [130/204]	Time  0.329 ( 0.154)	Loss 3.0000e+00 (3.0483e+00)	Acc@1   6.25 (  5.92)	Acc@5  37.50 ( 33.64)
Test: [140/204]	Time  0.150 ( 0.152)	Loss 2.7595e+00 (3.0428e+00)	Acc@1  12.50 (  6.07)	Acc@5  56.25 ( 33.87)
Test: [150/204]	Time  0.069 ( 0.148)	Loss 3.1130e+00 (3.0478e+00)	Acc@1   6.25 (  6.04)	Acc@5  37.50 ( 33.73)
Test: [160/204]	Time  0.067 ( 0.146)	Loss 3.1158e+00 (3.0539e+00)	Acc@1   6.25 (  5.98)	Acc@5  43.75 ( 33.54)
Test: [170/204]	Time  0.132 ( 0.144)	Loss 2.9449e+00 (3.0565e+00)	Acc@1  12.50 (  5.96)	Acc@5  37.50 ( 33.48)
Test: [180/204]	Time  0.219 ( 0.145)	Loss 3.0505e+00 (3.0515e+00)	Acc@1   6.25 (  5.94)	Acc@5  31.25 ( 33.56)
Test: [190/204]	Time  0.066 ( 0.145)	Loss 2.9045e+00 (3.0439e+00)	Acc@1   6.25 (  6.15)	Acc@5  37.50 ( 33.64)
Test: [200/204]	Time  0.089 ( 0.141)	Loss 3.0029e+00 (3.0393e+00)	Acc@1   0.00 (  6.00)	Acc@5  25.00 ( 33.61)
 * Acc@1 5.960 Acc@5 33.671
Epoch: [2][  0/812]	Time  7.820 ( 7.820)	Data  7.172 ( 7.172)	Loss 2.8015151023864746 (2.8015151023864746)	Acc@1  25.00 ( 25.00)	Acc@5  62.50 ( 62.50)
Epoch: [2][ 10/812]	Time  0.409 ( 1.243)	Data  0.001 ( 0.652)	Loss 3.0803389549255371 (2.9060559056021948)	Acc@1   6.25 ( 11.93)	Acc@5  37.50 ( 43.18)
Epoch: [2][ 20/812]	Time  0.276 ( 0.898)	Data  0.000 ( 0.342)	Loss 3.1186826229095459 (2.9526346411023821)	Acc@1   0.00 (  8.63)	Acc@5  50.00 ( 38.99)
Epoch: [2][ 30/812]	Time  0.322 ( 0.706)	Data  0.000 ( 0.232)	Loss 2.8857309818267822 (2.9662992646617274)	Acc@1   0.00 (  8.87)	Acc@5  37.50 ( 37.70)
Epoch: [2][ 40/812]	Time  0.321 ( 0.612)	Data  0.000 ( 0.175)	Loss 2.9538245201110840 (2.9774452302514054)	Acc@1  12.50 (  8.69)	Acc@5  37.50 ( 35.82)
Epoch: [2][ 50/812]	Time  0.621 ( 0.595)	Data  0.000 ( 0.141)	Loss 2.9366035461425781 (2.9699613720762965)	Acc@1   6.25 (  8.46)	Acc@5  37.50 ( 36.52)
Epoch: [2][ 60/812]	Time  0.363 ( 0.573)	Data  0.000 ( 0.118)	Loss 2.8158776760101318 (2.9805841328667815)	Acc@1   0.00 (  7.58)	Acc@5  37.50 ( 35.76)
Epoch: [2][ 70/812]	Time  1.140 ( 0.561)	Data  0.000 ( 0.102)	Loss 3.4899570941925049 (3.0041912139301568)	Acc@1   6.25 (  7.66)	Acc@5  12.50 ( 35.12)
Epoch: [2][ 80/812]	Time  0.319 ( 0.566)	Data  0.000 ( 0.089)	Loss 3.0084540843963623 (2.9994689152564531)	Acc@1   0.00 (  7.87)	Acc@5  37.50 ( 34.80)
Epoch: [2][ 90/812]	Time  0.268 ( 0.547)	Data  0.000 ( 0.080)	Loss 2.9627943038940430 (3.0006118423336154)	Acc@1  12.50 (  7.76)	Acc@5  37.50 ( 34.82)
Epoch: [2][100/812]	Time  0.316 ( 0.524)	Data  0.000 ( 0.072)	Loss 2.7565102577209473 (3.0043587330544348)	Acc@1  12.50 (  7.43)	Acc@5  43.75 ( 35.15)
Epoch: [2][110/812]	Time  0.429 ( 0.508)	Data  0.000 ( 0.065)	Loss 3.0070049762725830 (3.0021384148984342)	Acc@1  12.50 (  7.60)	Acc@5  25.00 ( 34.97)
Epoch: [2][120/812]	Time  0.305 ( 0.492)	Data  0.000 ( 0.060)	Loss 2.9685549736022949 (3.0040147403054984)	Acc@1   0.00 (  7.39)	Acc@5  43.75 ( 34.81)
Epoch: [2][130/812]	Time  0.283 ( 0.490)	Data  0.000 ( 0.055)	Loss 2.8806502819061279 (3.0021715236984137)	Acc@1   6.25 (  7.25)	Acc@5  37.50 ( 34.69)
Epoch: [2][140/812]	Time  0.771 ( 0.490)	Data  0.007 ( 0.051)	Loss 2.9726557731628418 (3.0007241516248553)	Acc@1   0.00 (  7.18)	Acc@5  37.50 ( 34.66)
Epoch: [2][150/812]	Time  0.288 ( 0.485)	Data  0.000 ( 0.048)	Loss 2.9617307186126709 (3.0027498804180826)	Acc@1   0.00 (  7.16)	Acc@5  25.00 ( 34.48)
Epoch: [2][160/812]	Time  0.478 ( 0.477)	Data  0.000 ( 0.045)	Loss 3.0584053993225098 (3.0030425483395593)	Acc@1  12.50 (  7.14)	Acc@5  37.50 ( 34.36)
Epoch: [2][170/812]	Time  0.263 ( 0.477)	Data  0.000 ( 0.043)	Loss 2.8512558937072754 (2.9933205027329293)	Acc@1  12.50 (  7.49)	Acc@5  50.00 ( 35.12)
Epoch: [2][180/812]	Time  0.346 ( 0.469)	Data  0.000 ( 0.040)	Loss 2.9997098445892334 (2.9925380477589139)	Acc@1   6.25 (  7.53)	Acc@5  50.00 ( 35.39)
Epoch: [2][190/812]	Time  0.413 ( 0.466)	Data  0.000 ( 0.038)	Loss 3.0204124450683594 (2.9885249599736396)	Acc@1  18.75 (  7.59)	Acc@5  37.50 ( 35.50)
Epoch: [2][200/812]	Time  0.413 ( 0.459)	Data  0.000 ( 0.036)	Loss 3.1792409420013428 (2.9872560797639154)	Acc@1   0.00 (  7.65)	Acc@5  25.00 ( 35.45)
Epoch: [2][210/812]	Time  0.438 ( 0.452)	Data  0.000 ( 0.034)	Loss 3.1474771499633789 (2.9855160735794719)	Acc@1   6.25 (  7.73)	Acc@5  31.25 ( 35.69)
Epoch: [2][220/812]	Time  0.491 ( 0.447)	Data  0.000 ( 0.033)	Loss 2.9277632236480713 (2.9854781983664673)	Acc@1   6.25 (  7.64)	Acc@5  37.50 ( 35.55)
Epoch: [2][230/812]	Time  0.305 ( 0.445)	Data  0.000 ( 0.032)	Loss 2.9457492828369141 (2.9842320035546375)	Acc@1   6.25 (  7.63)	Acc@5  50.00 ( 35.90)
Epoch: [2][240/812]	Time  0.323 ( 0.441)	Data  0.000 ( 0.030)	Loss 3.0428495407104492 (2.9817998735736513)	Acc@1   0.00 (  7.47)	Acc@5  31.25 ( 36.26)
Epoch: [2][250/812]	Time  0.301 ( 0.437)	Data  0.000 ( 0.029)	Loss 2.9430367946624756 (2.9817622866763536)	Acc@1   6.25 (  7.47)	Acc@5  31.25 ( 36.21)
Epoch: [2][260/812]	Time  0.586 ( 0.435)	Data  0.000 ( 0.028)	Loss 2.9803936481475830 (2.9805962308613276)	Acc@1   0.00 (  7.57)	Acc@5  25.00 ( 36.47)
Epoch: [2][270/812]	Time  0.299 ( 0.443)	Data  0.000 ( 0.027)	Loss 2.9111263751983643 (2.9786428736584654)	Acc@1   0.00 (  7.59)	Acc@5  18.75 ( 36.58)
Epoch: [2][280/812]	Time  0.341 ( 0.440)	Data  0.000 ( 0.026)	Loss 2.9854319095611572 (2.9766058938783257)	Acc@1   0.00 (  7.67)	Acc@5  43.75 ( 36.74)
Epoch: [2][290/812]	Time  0.329 ( 0.438)	Data  0.000 ( 0.025)	Loss 2.9319202899932861 (2.9726108259351802)	Acc@1  12.50 (  7.84)	Acc@5  37.50 ( 37.13)
Epoch: [2][300/812]	Time  0.446 ( 0.435)	Data  0.001 ( 0.024)	Loss 2.8546106815338135 (2.9693078899700378)	Acc@1   6.25 (  7.97)	Acc@5  37.50 ( 37.35)
Epoch: [2][310/812]	Time  0.341 ( 0.439)	Data  0.000 ( 0.024)	Loss 3.0818049907684326 (2.9699693117111057)	Acc@1   0.00 (  7.94)	Acc@5  31.25 ( 37.24)
Epoch: [2][320/812]	Time  0.274 ( 0.437)	Data  0.000 ( 0.023)	Loss 2.9243991374969482 (2.9688137706566451)	Acc@1  12.50 (  7.96)	Acc@5  31.25 ( 37.23)
Epoch: [2][330/812]	Time  0.338 ( 0.433)	Data  0.000 ( 0.022)	Loss 3.0309262275695801 (2.9696388885693970)	Acc@1   0.00 (  7.99)	Acc@5  31.25 ( 37.20)
Epoch: [2][340/812]	Time  0.389 ( 0.433)	Data  0.000 ( 0.021)	Loss 2.8249182701110840 (2.9681724110656469)	Acc@1   6.25 (  8.03)	Acc@5  37.50 ( 37.23)
Epoch: [2][350/812]	Time  0.310 ( 0.433)	Data  0.000 ( 0.021)	Loss 2.8536949157714844 (2.9678640820701578)	Acc@1  18.75 (  8.01)	Acc@5  43.75 ( 37.20)
Epoch: [2][360/812]	Time  0.268 ( 0.431)	Data  0.000 ( 0.020)	Loss 3.0409893989562988 (2.9666695891985273)	Acc@1   0.00 (  7.96)	Acc@5  37.50 ( 37.19)
Epoch: [2][370/812]	Time  0.261 ( 0.426)	Data  0.000 ( 0.020)	Loss 2.8385453224182129 (2.9652784012077311)	Acc@1   6.25 (  8.04)	Acc@5  50.00 ( 37.35)
Epoch: [2][380/812]	Time  0.332 ( 0.422)	Data  0.000 ( 0.019)	Loss 2.9035170078277588 (2.9619981059877891)	Acc@1   6.25 (  8.07)	Acc@5  37.50 ( 37.55)
Epoch: [2][390/812]	Time  0.277 ( 0.441)	Data  0.000 ( 0.040)	Loss 2.9315371513366699 (2.9634445335554038)	Acc@1  12.50 (  8.10)	Acc@5  37.50 ( 37.47)
Epoch: [2][400/812]	Time  0.379 ( 0.441)	Data  0.000 ( 0.039)	Loss 2.8441145420074463 (2.9616874506943245)	Acc@1   6.25 (  8.14)	Acc@5  37.50 ( 37.56)
Epoch: [2][410/812]	Time  1.566 ( 0.442)	Data  0.001 ( 0.038)	Loss 3.1246817111968994 (2.9628684630939270)	Acc@1   6.25 (  8.18)	Acc@5   6.25 ( 37.48)
Epoch: [2][420/812]	Time  0.279 ( 0.440)	Data  0.000 ( 0.037)	Loss 3.0193748474121094 (2.9639006182020462)	Acc@1   6.25 (  8.14)	Acc@5  37.50 ( 37.38)
Epoch: [2][430/812]	Time  0.351 ( 0.438)	Data  0.000 ( 0.036)	Loss 2.8867394924163818 (2.9645418014437861)	Acc@1  12.50 (  8.09)	Acc@5  56.25 ( 37.34)
Epoch: [2][440/812]	Time  0.310 ( 0.435)	Data  0.000 ( 0.035)	Loss 3.2345018386840820 (2.9647557454314630)	Acc@1   6.25 (  8.11)	Acc@5  25.00 ( 37.20)
Epoch: [2][450/812]	Time  0.462 ( 0.433)	Data  0.013 ( 0.034)	Loss 2.9480955600738525 (2.9646237594325369)	Acc@1  12.50 (  8.16)	Acc@5  31.25 ( 37.07)
Epoch: [2][460/812]	Time  0.361 ( 0.437)	Data  0.000 ( 0.034)	Loss 2.9906527996063232 (2.9641380118703120)	Acc@1   6.25 (  8.11)	Acc@5  37.50 ( 37.16)
Epoch: [2][470/812]	Time  0.320 ( 0.436)	Data  0.005 ( 0.033)	Loss 2.8664894104003906 (2.9636377306247721)	Acc@1   0.00 (  8.04)	Acc@5  43.75 ( 37.10)
Epoch: [2][480/812]	Time  0.728 ( 0.437)	Data  0.000 ( 0.032)	Loss 2.9491083621978760 (2.9644839109353365)	Acc@1   0.00 (  8.03)	Acc@5  37.50 ( 37.05)
Epoch: [2][490/812]	Time  0.449 ( 0.437)	Data  0.000 ( 0.032)	Loss 2.9922046661376953 (2.9635138370840224)	Acc@1  12.50 (  8.10)	Acc@5  37.50 ( 36.99)
Epoch: [2][500/812]	Time  0.603 ( 0.436)	Data  0.000 ( 0.031)	Loss 2.8569309711456299 (2.9627923232590603)	Acc@1  12.50 (  8.13)	Acc@5  31.25 ( 36.96)
Epoch: [2][510/812]	Time  0.749 ( 0.438)	Data  0.000 ( 0.030)	Loss 2.9989049434661865 (2.9636267118024731)	Acc@1  18.75 (  8.16)	Acc@5  25.00 ( 36.91)
Epoch: [2][520/812]	Time  0.328 ( 0.438)	Data  0.000 ( 0.030)	Loss 3.1219291687011719 (2.9637731618020906)	Acc@1   0.00 (  8.09)	Acc@5  25.00 ( 36.83)
Epoch: [2][530/812]	Time  0.283 ( 0.434)	Data  0.000 ( 0.029)	Loss 3.2799394130706787 (2.9658190481182323)	Acc@1   0.00 (  8.02)	Acc@5   6.25 ( 36.58)
Epoch: [2][540/812]	Time  0.392 ( 0.432)	Data  0.000 ( 0.029)	Loss 2.9686799049377441 (2.9679136465746021)	Acc@1  12.50 (  8.05)	Acc@5  43.75 ( 36.39)
Epoch: [2][550/812]	Time  0.314 ( 0.430)	Data  0.000 ( 0.028)	Loss 3.1606011390686035 (2.9685682713877268)	Acc@1   0.00 (  8.06)	Acc@5   6.25 ( 36.25)
Epoch: [2][560/812]	Time  0.304 ( 0.429)	Data  0.000 ( 0.028)	Loss 3.1311647891998291 (2.9694894771099940)	Acc@1  18.75 (  8.07)	Acc@5  18.75 ( 36.10)
Epoch: [2][570/812]	Time  0.296 ( 0.426)	Data  0.000 ( 0.027)	Loss 3.0004935264587402 (2.9696885679315144)	Acc@1   6.25 (  8.00)	Acc@5  37.50 ( 35.96)
Epoch: [2][580/812]	Time  0.402 ( 0.426)	Data  0.000 ( 0.027)	Loss 3.1540000438690186 (2.9704908922494178)	Acc@1   0.00 (  7.96)	Acc@5  31.25 ( 35.86)
Epoch: [2][590/812]	Time  0.355 ( 0.425)	Data  0.000 ( 0.026)	Loss 2.8776111602783203 (2.9706806236919010)	Acc@1   0.00 (  7.88)	Acc@5  37.50 ( 35.76)
Epoch: [2][600/812]	Time  0.339 ( 0.427)	Data  0.000 ( 0.026)	Loss 3.1633343696594238 (2.9714494632206820)	Acc@1   6.25 (  7.88)	Acc@5  31.25 ( 35.66)
Epoch: [2][610/812]	Time  0.456 ( 0.426)	Data  0.000 ( 0.026)	Loss 3.0913407802581787 (2.9723785126462894)	Acc@1   0.00 (  7.90)	Acc@5  12.50 ( 35.53)
Epoch: [2][620/812]	Time  0.465 ( 0.427)	Data  0.000 ( 0.025)	Loss 2.8533611297607422 (2.9729234615577784)	Acc@1   6.25 (  7.84)	Acc@5  37.50 ( 35.38)
Epoch: [2][630/812]	Time  0.347 ( 0.426)	Data  0.000 ( 0.025)	Loss 3.0750308036804199 (2.9732160844062285)	Acc@1  12.50 (  7.82)	Acc@5  25.00 ( 35.39)
Epoch: [2][640/812]	Time  0.928 ( 0.428)	Data  0.000 ( 0.024)	Loss 2.9126179218292236 (2.9739369267420539)	Acc@1  25.00 (  7.85)	Acc@5  37.50 ( 35.34)
Epoch: [2][650/812]	Time  0.355 ( 0.427)	Data  0.000 ( 0.024)	Loss 3.2829296588897705 (2.9742086918855777)	Acc@1   0.00 (  7.81)	Acc@5   0.00 ( 35.26)
Epoch: [2][660/812]	Time  0.523 ( 0.430)	Data  0.000 ( 0.024)	Loss 2.9777669906616211 (2.9746465405249198)	Acc@1   0.00 (  7.80)	Acc@5  31.25 ( 35.24)
Epoch: [2][670/812]	Time  0.575 ( 0.429)	Data  0.001 ( 0.023)	Loss 2.9968461990356445 (2.9754822129819503)	Acc@1   0.00 (  7.72)	Acc@5  37.50 ( 35.08)
Epoch: [2][680/812]	Time  0.365 ( 0.433)	Data  0.000 ( 0.023)	Loss 3.0973372459411621 (2.9756118806623326)	Acc@1   0.00 (  7.69)	Acc@5  18.75 ( 35.01)
Epoch: [2][690/812]	Time  0.357 ( 0.432)	Data  0.000 ( 0.023)	Loss 2.9333236217498779 (2.9752553789384457)	Acc@1   6.25 (  7.72)	Acc@5  31.25 ( 35.03)
Epoch: [2][700/812]	Time  0.462 ( 0.432)	Data  0.000 ( 0.022)	Loss 2.8496241569519043 (2.9756184831665520)	Acc@1   6.25 (  7.70)	Acc@5  43.75 ( 34.99)
Epoch: [2][710/812]	Time  0.345 ( 0.431)	Data  0.000 ( 0.022)	Loss 2.9835886955261230 (2.9758846763149429)	Acc@1  12.50 (  7.71)	Acc@5  37.50 ( 34.93)
Epoch: [2][720/812]	Time  0.314 ( 0.429)	Data  0.000 ( 0.022)	Loss 3.0218732357025146 (2.9757975037319486)	Acc@1   6.25 (  7.66)	Acc@5  31.25 ( 34.86)
Epoch: [2][730/812]	Time  0.298 ( 0.429)	Data  0.000 ( 0.021)	Loss 2.9064211845397949 (2.9769096097280814)	Acc@1   6.25 (  7.64)	Acc@5  18.75 ( 34.70)
Epoch: [2][740/812]	Time  0.359 ( 0.427)	Data  0.000 ( 0.021)	Loss 2.9737784862518311 (2.9778640643466017)	Acc@1  18.75 (  7.61)	Acc@5  56.25 ( 34.63)
Epoch: [2][750/812]	Time  0.350 ( 0.426)	Data  0.000 ( 0.021)	Loss 2.9970128536224365 (2.9777942691122328)	Acc@1   6.25 (  7.58)	Acc@5  43.75 ( 34.60)
Epoch: [2][760/812]	Time  0.321 ( 0.425)	Data  0.000 ( 0.021)	Loss 2.9020175933837891 (2.9782564702704453)	Acc@1  18.75 (  7.60)	Acc@5  50.00 ( 34.59)
Epoch: [2][770/812]	Time  0.369 ( 0.423)	Data  0.000 ( 0.020)	Loss 2.9848797321319580 (2.9784796457500926)	Acc@1  18.75 (  7.56)	Acc@5  25.00 ( 34.55)
Epoch: [2][780/812]	Time  0.355 ( 0.422)	Data  0.000 ( 0.020)	Loss 3.0853300094604492 (2.9788403755266137)	Acc@1   6.25 (  7.54)	Acc@5  18.75 ( 34.49)
Epoch: [2][790/812]	Time  0.315 ( 0.421)	Data  0.000 ( 0.020)	Loss 3.0598042011260986 (2.9789923198907324)	Acc@1   0.00 (  7.53)	Acc@5  31.25 ( 34.49)
Epoch: [2][800/812]	Time  0.372 ( 0.420)	Data  0.000 ( 0.020)	Loss 3.0041913986206055 (2.9789370779092241)	Acc@1   6.25 (  7.53)	Acc@5  25.00 ( 34.47)
Epoch: [2][810/812]	Time  0.281 ( 0.419)	Data  0.000 ( 0.019)	Loss 2.9729373455047607 (2.9792467001481060)	Acc@1   0.00 (  7.49)	Acc@5  50.00 ( 34.46)
epoch: 2, Avg_Loss 2.9796732561341646
Test: [  0/204]	Time  2.652 ( 2.652)	Loss 3.0625e+00 (3.0625e+00)	Acc@1   6.25 (  6.25)	Acc@5  31.25 ( 31.25)
Test: [ 10/204]	Time  0.124 ( 0.340)	Loss 2.8932e+00 (3.1745e+00)	Acc@1   6.25 (  8.52)	Acc@5  56.25 ( 38.07)
Test: [ 20/204]	Time  0.116 ( 0.233)	Loss 2.9529e+00 (3.1226e+00)	Acc@1   6.25 (  6.85)	Acc@5  18.75 ( 33.63)
Test: [ 30/204]	Time  0.099 ( 0.196)	Loss 2.8848e+00 (3.1210e+00)	Acc@1   6.25 (  6.25)	Acc@5  43.75 ( 31.65)
Test: [ 40/204]	Time  0.114 ( 0.199)	Loss 3.0256e+00 (3.1284e+00)	Acc@1  18.75 (  5.95)	Acc@5  56.25 ( 31.55)
Test: [ 50/204]	Time  0.470 ( 0.202)	Loss 3.3755e+00 (3.1556e+00)	Acc@1   0.00 (  5.51)	Acc@5  12.50 ( 29.53)
Test: [ 60/204]	Time  0.228 ( 0.218)	Loss 3.1169e+00 (3.1430e+00)	Acc@1  12.50 (  5.94)	Acc@5  25.00 ( 29.61)
Test: [ 70/204]	Time  0.057 ( 0.200)	Loss 3.1988e+00 (3.1595e+00)	Acc@1   6.25 (  6.25)	Acc@5  31.25 ( 29.40)
Test: [ 80/204]	Time  0.057 ( 0.184)	Loss 3.1687e+00 (3.1644e+00)	Acc@1  12.50 (  6.40)	Acc@5  12.50 ( 29.17)
Test: [ 90/204]	Time  0.085 ( 0.174)	Loss 3.0910e+00 (3.1564e+00)	Acc@1   0.00 (  6.59)	Acc@5  18.75 ( 29.12)
Test: [100/204]	Time  0.174 ( 0.167)	Loss 3.0543e+00 (3.1564e+00)	Acc@1   0.00 (  6.68)	Acc@5  43.75 ( 30.07)
Test: [110/204]	Time  0.062 ( 0.162)	Loss 2.9828e+00 (3.1624e+00)	Acc@1   0.00 (  6.59)	Acc@5  31.25 ( 29.84)
Test: [120/204]	Time  0.066 ( 0.156)	Loss 3.1331e+00 (3.1570e+00)	Acc@1  12.50 (  6.30)	Acc@5  43.75 ( 29.86)
Test: [130/204]	Time  0.141 ( 0.151)	Loss 3.4151e+00 (3.1601e+00)	Acc@1  12.50 (  6.44)	Acc@5  37.50 ( 30.01)
Test: [140/204]	Time  0.254 ( 0.148)	Loss 3.4069e+00 (3.1633e+00)	Acc@1   0.00 (  6.43)	Acc@5  12.50 ( 29.57)
Test: [150/204]	Time  0.167 ( 0.150)	Loss 3.0685e+00 (3.1651e+00)	Acc@1   6.25 (  6.29)	Acc@5  31.25 ( 29.22)
Test: [160/204]	Time  0.183 ( 0.151)	Loss 3.0397e+00 (3.1617e+00)	Acc@1  12.50 (  6.41)	Acc@5  31.25 ( 29.15)
Test: [170/204]	Time  0.060 ( 0.148)	Loss 2.8078e+00 (3.1622e+00)	Acc@1   6.25 (  6.43)	Acc@5  37.50 ( 29.24)
Test: [180/204]	Time  0.057 ( 0.143)	Loss 3.2692e+00 (3.1654e+00)	Acc@1   0.00 (  6.42)	Acc@5  37.50 ( 29.28)
Test: [190/204]	Time  0.067 ( 0.140)	Loss 3.1718e+00 (3.1664e+00)	Acc@1   0.00 (  6.48)	Acc@5  18.75 ( 29.06)
Test: [200/204]	Time  0.058 ( 0.137)	Loss 3.6892e+00 (3.1707e+00)	Acc@1   0.00 (  6.34)	Acc@5  25.00 ( 29.14)
 * Acc@1 6.298 Acc@5 28.940
Epoch: [3][  0/812]	Time  5.724 ( 5.724)	Data  5.113 ( 5.113)	Loss 3.0158214569091797 (3.0158214569091797)	Acc@1   0.00 (  0.00)	Acc@5  25.00 ( 25.00)
Epoch: [3][ 10/812]	Time  0.334 ( 1.032)	Data  0.001 ( 0.465)	Loss 3.0833282470703125 (3.0264479030262339)	Acc@1   6.25 (  5.68)	Acc@5  25.00 ( 27.27)
Epoch: [3][ 20/812]	Time  2.552 ( 0.867)	Data  0.000 ( 0.244)	Loss 2.9120440483093262 (3.0448687984829856)	Acc@1   0.00 (  4.46)	Acc@5  43.75 ( 26.79)
Epoch: [3][ 30/812]	Time  0.342 ( 0.816)	Data  0.000 ( 0.165)	Loss 3.1349847316741943 (3.0172335870804323)	Acc@1   6.25 (  6.25)	Acc@5  18.75 ( 28.02)
Epoch: [3][ 40/812]	Time  1.249 ( 0.915)	Data  0.000 ( 0.126)	Loss 2.9883620738983154 (3.0210179468480551)	Acc@1   0.00 (  6.86)	Acc@5  37.50 ( 28.66)
Epoch: [3][ 50/812]	Time  0.367 ( 0.901)	Data  0.000 ( 0.101)	Loss 2.9883222579956055 (3.0159403623319139)	Acc@1   6.25 (  7.60)	Acc@5  25.00 ( 29.41)
Epoch: [3][ 60/812]	Time  0.465 ( 0.828)	Data  0.000 ( 0.085)	Loss 2.8750565052032471 (3.0183911636227467)	Acc@1  12.50 (  6.97)	Acc@5  25.00 ( 28.79)
Epoch: [3][ 70/812]	Time  0.424 ( 0.778)	Data  0.000 ( 0.073)	Loss 2.9494729042053223 (3.0145486106335277)	Acc@1   0.00 (  7.13)	Acc@5  25.00 ( 29.23)
Epoch: [3][ 80/812]	Time  0.383 ( 0.766)	Data  0.000 ( 0.064)	Loss 3.0335714817047119 (3.0193263307029818)	Acc@1  12.50 (  6.94)	Acc@5  37.50 ( 29.01)
Epoch: [3][ 90/812]	Time  0.339 ( 0.775)	Data  0.000 ( 0.057)	Loss 2.9148027896881104 (3.0196929947360531)	Acc@1   0.00 (  6.66)	Acc@5  50.00 ( 29.19)
Epoch: [3][100/812]	Time  0.381 ( 0.733)	Data  0.000 ( 0.051)	Loss 3.0561730861663818 (3.0198501289480983)	Acc@1   0.00 (  6.50)	Acc@5  12.50 ( 28.84)
Epoch: [3][110/812]	Time  0.337 ( 0.700)	Data  0.000 ( 0.047)	Loss 2.9496860504150391 (3.0230071888313637)	Acc@1   0.00 (  6.25)	Acc@5  25.00 ( 28.66)
Epoch: [3][120/812]	Time  0.312 ( 0.668)	Data  0.000 ( 0.043)	Loss 3.2124702930450439 (3.0222964030652006)	Acc@1   6.25 (  6.04)	Acc@5  25.00 ( 28.51)
Epoch: [3][130/812]	Time  0.309 ( 0.641)	Data  0.000 ( 0.040)	Loss 3.0430448055267334 (3.0221449528031679)	Acc@1   6.25 (  5.96)	Acc@5  31.25 ( 28.58)
Epoch: [3][140/812]	Time  0.527 ( 0.628)	Data  0.000 ( 0.037)	Loss 2.8736240863800049 (3.0212468306223550)	Acc@1   6.25 (  5.94)	Acc@5  50.00 ( 28.63)
Epoch: [3][150/812]	Time  1.196 ( 0.656)	Data  0.002 ( 0.034)	Loss 2.9610450267791748 (3.0214170999084877)	Acc@1   0.00 (  5.88)	Acc@5  25.00 ( 28.85)
Epoch: [3][160/812]	Time  0.422 ( 0.656)	Data  0.000 ( 0.032)	Loss 3.1517665386199951 (3.0216846258743950)	Acc@1   0.00 (  5.82)	Acc@5  18.75 ( 28.69)
Epoch: [3][170/812]	Time  0.269 ( 0.642)	Data  0.000 ( 0.030)	Loss 3.0571994781494141 (3.0228977412508247)	Acc@1   6.25 (  5.77)	Acc@5  25.00 ( 28.65)
Epoch: [3][180/812]	Time  0.269 ( 0.622)	Data  0.000 ( 0.029)	Loss 2.9730086326599121 (3.0199726062584977)	Acc@1   0.00 (  5.59)	Acc@5  18.75 ( 28.83)
Epoch: [3][190/812]	Time  0.383 ( 0.614)	Data  0.000 ( 0.027)	Loss 3.0246059894561768 (3.0179857933084380)	Acc@1   6.25 (  5.66)	Acc@5  18.75 ( 28.96)
Epoch: [3][200/812]	Time  0.266 ( 0.620)	Data  0.000 ( 0.026)	Loss 2.9241554737091064 (3.0154628184304308)	Acc@1   0.00 (  5.57)	Acc@5  43.75 ( 29.29)
Epoch: [3][210/812]	Time  0.601 ( 0.611)	Data  0.000 ( 0.025)	Loss 3.0757744312286377 (3.0144418178576431)	Acc@1   6.25 (  5.51)	Acc@5  18.75 ( 29.21)
Epoch: [3][220/812]	Time  0.665 ( 0.624)	Data  0.000 ( 0.024)	Loss 3.0334370136260986 (3.0150288143848401)	Acc@1  18.75 (  5.66)	Acc@5  50.00 ( 29.13)
Epoch: [3][230/812]	Time  0.357 ( 0.617)	Data  0.000 ( 0.023)	Loss 2.9380850791931152 (3.0177218573434010)	Acc@1   6.25 (  5.63)	Acc@5  31.25 ( 29.09)
Epoch: [3][240/812]	Time  0.587 ( 0.634)	Data  0.000 ( 0.022)	Loss 2.9971942901611328 (3.0166552729626415)	Acc@1  12.50 (  5.73)	Acc@5  31.25 ( 29.33)
Epoch: [3][250/812]	Time  0.347 ( 0.630)	Data  0.000 ( 0.021)	Loss 2.9815535545349121 (3.0146788106971529)	Acc@1  12.50 (  5.78)	Acc@5  50.00 ( 29.46)
Epoch: [3][260/812]	Time  0.265 ( 0.620)	Data  0.000 ( 0.020)	Loss 3.0071671009063721 (3.0132315305000978)	Acc@1   6.25 (  5.82)	Acc@5  37.50 ( 29.67)
Epoch: [3][270/812]	Time  0.658 ( 0.613)	Data  0.000 ( 0.019)	Loss 3.1202378273010254 (3.0120008950743729)	Acc@1  18.75 (  5.93)	Acc@5  31.25 ( 29.84)
Epoch: [3][280/812]	Time  0.291 ( 0.648)	Data  0.000 ( 0.019)	Loss 2.9311268329620361 (3.0108902505284103)	Acc@1   0.00 (  5.94)	Acc@5  56.25 ( 30.03)
Epoch: [3][290/812]	Time  0.456 ( 0.639)	Data  0.000 ( 0.018)	Loss 2.9996328353881836 (3.0105642535022854)	Acc@1   6.25 (  5.99)	Acc@5  31.25 ( 30.11)
Epoch: [3][300/812]	Time  0.512 ( 0.639)	Data  0.000 ( 0.018)	Loss 2.9348669052124023 (3.0079215927377492)	Acc@1   6.25 (  6.19)	Acc@5  43.75 ( 30.44)
Epoch: [3][310/812]	Time  0.393 ( 0.630)	Data  0.000 ( 0.017)	Loss 2.8758938312530518 (3.0068881427360119)	Acc@1  18.75 (  6.29)	Acc@5  31.25 ( 30.69)
Epoch: [3][320/812]	Time  2.239 ( 0.638)	Data  0.001 ( 0.017)	Loss 3.0526385307312012 (3.0063136856875317)	Acc@1   0.00 (  6.31)	Acc@5  25.00 ( 30.80)
Epoch: [3][330/812]	Time  1.655 ( 0.643)	Data  0.001 ( 0.016)	Loss 3.1010005474090576 (3.0071979944799603)	Acc@1   6.25 (  6.33)	Acc@5  12.50 ( 30.66)
Epoch: [3][340/812]	Time  0.354 ( 0.640)	Data  0.000 ( 0.016)	Loss 2.8616697788238525 (3.0074089762052831)	Acc@1   6.25 (  6.38)	Acc@5  50.00 ( 30.96)
Epoch: [3][350/812]	Time  0.262 ( 0.631)	Data  0.000 ( 0.015)	Loss 2.8579890727996826 (3.0067842495747104)	Acc@1  12.50 (  6.41)	Acc@5  37.50 ( 30.97)
Epoch: [3][360/812]	Time  0.264 ( 0.621)	Data  0.000 ( 0.015)	Loss 2.8985655307769775 (3.0060285617109810)	Acc@1   6.25 (  6.39)	Acc@5  31.25 ( 30.75)
Epoch: [3][370/812]	Time  0.431 ( 0.644)	Data  0.000 ( 0.036)	Loss 3.0459122657775879 (3.0049481848179491)	Acc@1   6.25 (  6.44)	Acc@5  12.50 ( 30.86)
Epoch: [3][380/812]	Time  0.314 ( 0.636)	Data  0.000 ( 0.035)	Loss 3.0395123958587646 (3.0038182754216232)	Acc@1  12.50 (  6.45)	Acc@5  25.00 ( 30.92)
Epoch: [3][390/812]	Time  0.279 ( 0.628)	Data  0.000 ( 0.034)	Loss 2.9086861610412598 (3.0028320974706078)	Acc@1  25.00 (  6.47)	Acc@5  50.00 ( 31.14)
Epoch: [3][400/812]	Time  0.392 ( 0.621)	Data  0.000 ( 0.033)	Loss 2.9089100360870361 (3.0020880122434468)	Acc@1  12.50 (  6.53)	Acc@5  50.00 ( 31.25)
Epoch: [3][410/812]	Time  0.329 ( 0.621)	Data  0.000 ( 0.032)	Loss 2.9081573486328125 (3.0012173861482716)	Acc@1   0.00 (  6.54)	Acc@5  31.25 ( 31.19)
Epoch: [3][420/812]	Time  0.513 ( 0.616)	Data  0.000 ( 0.032)	Loss 2.8722672462463379 (2.9999153534760103)	Acc@1  18.75 (  6.62)	Acc@5  43.75 ( 31.31)
Epoch: [3][430/812]	Time  0.345 ( 0.616)	Data  0.000 ( 0.031)	Loss 3.1053504943847656 (3.0002358219739733)	Acc@1   6.25 (  6.66)	Acc@5  25.00 ( 31.32)
Epoch: [3][440/812]	Time  5.932 ( 0.625)	Data  0.001 ( 0.030)	Loss 2.9023909568786621 (2.9991509211576983)	Acc@1   6.25 (  6.63)	Acc@5  50.00 ( 31.49)
Epoch: [3][450/812]	Time  0.956 ( 0.625)	Data  0.007 ( 0.030)	Loss 2.9840891361236572 (2.9981299747120249)	Acc@1   6.25 (  6.67)	Acc@5  37.50 ( 31.61)
Epoch: [3][460/812]	Time  0.315 ( 0.621)	Data  0.004 ( 0.029)	Loss 2.9369568824768066 (2.9980495519079509)	Acc@1  12.50 (  6.74)	Acc@5  31.25 ( 31.71)
Epoch: [3][470/812]	Time  0.301 ( 0.614)	Data  0.000 ( 0.028)	Loss 3.0156629085540771 (2.9977203802444881)	Acc@1   0.00 (  6.69)	Acc@5  18.75 ( 31.74)
Epoch: [3][480/812]	Time  3.321 ( 0.619)	Data  0.001 ( 0.028)	Loss 2.8487274646759033 (2.9971513133534771)	Acc@1  18.75 (  6.76)	Acc@5  50.00 ( 31.80)
Epoch: [3][490/812]	Time  0.313 ( 0.612)	Data  0.000 ( 0.027)	Loss 3.0396497249603271 (2.9978236030415459)	Acc@1   0.00 (  6.76)	Acc@5  18.75 ( 31.71)
Epoch: [3][500/812]	Time  0.423 ( 0.609)	Data  0.001 ( 0.027)	Loss 3.0073096752166748 (2.9980987507901982)	Acc@1   6.25 (  6.75)	Acc@5  18.75 ( 31.62)
Epoch: [3][510/812]	Time  0.434 ( 0.617)	Data  0.000 ( 0.026)	Loss 3.0058531761169434 (2.9978680363373282)	Acc@1  12.50 (  6.76)	Acc@5  37.50 ( 31.71)
Epoch: [3][520/812]	Time  0.476 ( 0.613)	Data  0.000 ( 0.026)	Loss 2.9997620582580566 (2.9974940948889031)	Acc@1   6.25 (  6.78)	Acc@5  31.25 ( 31.77)
Epoch: [3][530/812]	Time  0.294 ( 0.615)	Data  0.000 ( 0.025)	Loss 2.8541901111602783 (2.9975931828529374)	Acc@1  12.50 (  6.79)	Acc@5  31.25 ( 31.79)
Epoch: [3][540/812]	Time  0.453 ( 0.611)	Data  0.000 ( 0.025)	Loss 3.0939049720764160 (2.9975297812393102)	Acc@1  12.50 (  6.77)	Acc@5  31.25 ( 31.76)
Epoch: [3][550/812]	Time  0.351 ( 0.614)	Data  0.000 ( 0.024)	Loss 2.9648787975311279 (2.9976563155110214)	Acc@1  12.50 (  6.78)	Acc@5  31.25 ( 31.78)
Epoch: [3][560/812]	Time  0.320 ( 0.609)	Data  0.000 ( 0.024)	Loss 3.1519083976745605 (2.9972702305159680)	Acc@1  12.50 (  6.85)	Acc@5  31.25 ( 31.82)
Epoch: [3][570/812]	Time  0.275 ( 0.618)	Data  0.000 ( 0.023)	Loss 2.9707462787628174 (2.9971992213755274)	Acc@1   6.25 (  6.90)	Acc@5  31.25 ( 31.82)
Epoch: [3][580/812]	Time  0.288 ( 0.612)	Data  0.000 ( 0.023)	Loss 3.1000468730926514 (2.9966069059815137)	Acc@1   6.25 (  6.92)	Acc@5  12.50 ( 31.78)
Epoch: [3][590/812]	Time  0.585 ( 0.609)	Data  0.000 ( 0.023)	Loss 2.9190781116485596 (2.9951890721135537)	Acc@1   0.00 (  6.96)	Acc@5  43.75 ( 31.92)
Epoch: [3][600/812]	Time  0.350 ( 0.608)	Data  0.000 ( 0.022)	Loss 3.0638854503631592 (2.9946094166220920)	Acc@1   6.25 (  7.02)	Acc@5  25.00 ( 31.95)
Epoch: [3][610/812]	Time  0.426 ( 0.611)	Data  0.000 ( 0.022)	Loss 3.0002925395965576 (2.9947646444635967)	Acc@1  12.50 (  7.06)	Acc@5  50.00 ( 31.96)
Epoch: [3][620/812]	Time  0.309 ( 0.613)	Data  0.000 ( 0.022)	Loss 2.7509143352508545 (2.9938925218658938)	Acc@1  18.75 (  7.09)	Acc@5  43.75 ( 31.94)
Epoch: [3][630/812]	Time  0.806 ( 0.610)	Data  0.000 ( 0.021)	Loss 2.8680553436279297 (2.9924048750222956)	Acc@1  18.75 (  7.14)	Acc@5  37.50 ( 32.12)
Epoch: [3][640/812]	Time  0.386 ( 0.614)	Data  0.000 ( 0.021)	Loss 2.9069545269012451 (2.9924681841304261)	Acc@1   6.25 (  7.14)	Acc@5  50.00 ( 32.24)
Epoch: [3][650/812]	Time  0.607 ( 0.613)	Data  0.000 ( 0.021)	Loss 2.9802098274230957 (2.9918146008903164)	Acc@1  12.50 (  7.19)	Acc@5  37.50 ( 32.30)
Epoch: [3][660/812]	Time  0.930 ( 0.616)	Data  0.000 ( 0.020)	Loss 3.2394642829895020 (2.9911244673014767)	Acc@1   0.00 (  7.22)	Acc@5   6.25 ( 32.35)
Epoch: [3][670/812]	Time  0.639 ( 0.621)	Data  0.000 ( 0.020)	Loss 3.0861592292785645 (2.9900799476265374)	Acc@1   6.25 (  7.26)	Acc@5  12.50 ( 32.38)
Epoch: [3][680/812]	Time  0.275 ( 0.620)	Data  0.000 ( 0.020)	Loss 3.2106945514678955 (2.9891572772318744)	Acc@1   6.25 (  7.34)	Acc@5  12.50 ( 32.50)
Epoch: [3][690/812]	Time  0.555 ( 0.616)	Data  0.000 ( 0.019)	Loss 2.9980182647705078 (2.9897532028330733)	Acc@1  12.50 (  7.34)	Acc@5  31.25 ( 32.42)
Epoch: [3][700/812]	Time  0.653 ( 0.614)	Data  0.000 ( 0.019)	Loss 2.9801723957061768 (2.9899968665608667)	Acc@1  12.50 (  7.32)	Acc@5  50.00 ( 32.44)
Epoch: [3][710/812]	Time  0.662 ( 0.613)	Data  0.000 ( 0.019)	Loss 2.8820955753326416 (2.9891230261946196)	Acc@1   6.25 (  7.41)	Acc@5  50.00 ( 32.54)
Epoch: [3][720/812]	Time  0.354 ( 0.616)	Data  0.000 ( 0.019)	Loss 3.1623504161834717 (2.9880944088659405)	Acc@1   6.25 (  7.47)	Acc@5  18.75 ( 32.61)
Epoch: [3][730/812]	Time  0.405 ( 0.613)	Data  0.000 ( 0.018)	Loss 3.0475122928619385 (2.9880064864491307)	Acc@1  12.50 (  7.43)	Acc@5  31.25 ( 32.67)
Epoch: [3][740/812]	Time  0.854 ( 0.614)	Data  0.001 ( 0.018)	Loss 2.9978206157684326 (2.9881713860108947)	Acc@1  18.75 (  7.43)	Acc@5  37.50 ( 32.67)
Epoch: [3][750/812]	Time  2.210 ( 0.620)	Data  0.001 ( 0.018)	Loss 2.7746357917785645 (2.9868896112302332)	Acc@1  18.75 (  7.46)	Acc@5  50.00 ( 32.75)
Epoch: [3][760/812]	Time  0.380 ( 0.621)	Data  0.000 ( 0.018)	Loss 3.2500312328338623 (2.9868910519428229)	Acc@1   0.00 (  7.45)	Acc@5  31.25 ( 32.79)
Epoch: [3][770/812]	Time  0.252 ( 0.620)	Data  0.000 ( 0.018)	Loss 2.8388483524322510 (2.9866402096630842)	Acc@1   0.00 (  7.46)	Acc@5  43.75 ( 32.82)
Epoch: [3][780/812]	Time  0.260 ( 0.617)	Data  0.000 ( 0.017)	Loss 2.9738409519195557 (2.9865102337508747)	Acc@1   6.25 (  7.45)	Acc@5  37.50 ( 32.84)
Epoch: [3][790/812]	Time  0.901 ( 0.614)	Data  0.000 ( 0.017)	Loss 3.0940434932708740 (2.9866778401448362)	Acc@1   0.00 (  7.47)	Acc@5  18.75 ( 32.85)
Epoch: [3][800/812]	Time  0.879 ( 0.619)	Data  0.000 ( 0.017)	Loss 3.0417487621307373 (2.9859244793690696)	Acc@1   6.25 (  7.50)	Acc@5  43.75 ( 32.94)
Epoch: [3][810/812]	Time  0.321 ( 0.620)	Data  0.000 ( 0.017)	Loss 2.9316492080688477 (2.9845150692983267)	Acc@1   6.25 (  7.48)	Acc@5  43.75 ( 33.05)
epoch: 3, Avg_Loss 2.98457957664734
Test: [  0/204]	Time  2.876 ( 2.876)	Loss 3.2005e+00 (3.2005e+00)	Acc@1   0.00 (  0.00)	Acc@5  18.75 ( 18.75)
Test: [ 10/204]	Time  0.073 ( 0.857)	Loss 2.9082e+00 (3.1084e+00)	Acc@1   0.00 (  8.52)	Acc@5  43.75 ( 38.07)
Test: [ 20/204]	Time  0.057 ( 0.485)	Loss 3.0790e+00 (3.0726e+00)	Acc@1  12.50 (  9.23)	Acc@5  18.75 ( 35.71)
Test: [ 30/204]	Time  0.154 ( 0.353)	Loss 3.0536e+00 (3.0419e+00)	Acc@1  18.75 ( 10.48)	Acc@5  31.25 ( 36.49)
Test: [ 40/204]	Time  0.080 ( 0.294)	Loss 3.0361e+00 (3.0447e+00)	Acc@1  18.75 (  9.45)	Acc@5  50.00 ( 37.20)
Test: [ 50/204]	Time  0.154 ( 0.262)	Loss 3.1160e+00 (3.0430e+00)	Acc@1   6.25 (  9.19)	Acc@5  25.00 ( 36.40)
Test: [ 60/204]	Time  0.077 ( 0.239)	Loss 3.7089e+00 (3.0569e+00)	Acc@1   0.00 (  9.12)	Acc@5  12.50 ( 37.30)
Test: [ 70/204]	Time  0.095 ( 0.217)	Loss 3.1275e+00 (3.0871e+00)	Acc@1   6.25 (  8.80)	Acc@5  18.75 ( 36.27)
Test: [ 80/204]	Time  0.250 ( 0.221)	Loss 3.1932e+00 (3.0789e+00)	Acc@1   6.25 (  8.95)	Acc@5  37.50 ( 36.73)
Test: [ 90/204]	Time  0.289 ( 0.235)	Loss 3.2581e+00 (3.0892e+00)	Acc@1   0.00 (  8.79)	Acc@5  12.50 ( 35.99)
Test: [100/204]	Time  0.285 ( 0.228)	Loss 2.9315e+00 (3.0842e+00)	Acc@1   0.00 (  8.54)	Acc@5  43.75 ( 36.01)
Test: [110/204]	Time  0.123 ( 0.284)	Loss 3.1104e+00 (3.0914e+00)	Acc@1   0.00 (  8.56)	Acc@5  18.75 ( 35.47)
Test: [120/204]	Time  0.183 ( 0.272)	Loss 2.8803e+00 (3.0881e+00)	Acc@1   0.00 (  8.26)	Acc@5  56.25 ( 35.49)
Test: [130/204]	Time  0.144 ( 0.262)	Loss 2.8756e+00 (3.0880e+00)	Acc@1  12.50 (  8.21)	Acc@5  50.00 ( 35.26)
Test: [140/204]	Time  0.102 ( 0.250)	Loss 3.4670e+00 (3.0885e+00)	Acc@1   6.25 (  8.29)	Acc@5  25.00 ( 35.06)
Test: [150/204]	Time  0.096 ( 0.241)	Loss 3.6204e+00 (3.0929e+00)	Acc@1   0.00 (  8.07)	Acc@5  31.25 ( 35.18)
Test: [160/204]	Time  0.125 ( 0.235)	Loss 3.0912e+00 (3.0904e+00)	Acc@1   0.00 (  8.00)	Acc@5  31.25 ( 35.17)
Test: [170/204]	Time  0.143 ( 0.229)	Loss 3.2768e+00 (3.0918e+00)	Acc@1   6.25 (  7.86)	Acc@5  25.00 ( 35.16)
Test: [180/204]	Time  0.332 ( 0.229)	Loss 3.0591e+00 (3.1056e+00)	Acc@1  12.50 (  7.84)	Acc@5  37.50 ( 34.67)
Test: [190/204]	Time  0.089 ( 0.226)	Loss 2.7978e+00 (3.1094e+00)	Acc@1  12.50 (  7.89)	Acc@5  62.50 ( 34.78)
Test: [200/204]	Time  0.056 ( 0.218)	Loss 3.0167e+00 (3.1102e+00)	Acc@1   0.00 (  7.77)	Acc@5  18.75 ( 34.61)
 * Acc@1 7.773 Acc@5 34.470
Epoch: [4][  0/812]	Time  7.448 ( 7.448)	Data  5.173 ( 5.173)	Loss 2.7923336029052734 (2.7923336029052734)	Acc@1  12.50 ( 12.50)	Acc@5  37.50 ( 37.50)
Epoch: [4][ 10/812]	Time  0.257 ( 0.986)	Data  0.001 ( 0.471)	Loss 3.0314331054687500 (3.0026354572989722)	Acc@1   0.00 (  9.66)	Acc@5  12.50 ( 33.52)
Epoch: [4][ 20/812]	Time  0.324 ( 0.662)	Data  0.000 ( 0.247)	Loss 2.8808708190917969 (2.9995421795617965)	Acc@1   6.25 (  8.33)	Acc@5  37.50 ( 35.12)
Epoch: [4][ 30/812]	Time  0.336 ( 0.552)	Data  0.000 ( 0.167)	Loss 3.0033607482910156 (2.9814699772865541)	Acc@1   6.25 (  7.86)	Acc@5  37.50 ( 36.90)
Epoch: [4][ 40/812]	Time  0.497 ( 0.539)	Data  0.020 ( 0.127)	Loss 3.2765998840332031 (2.9698352522966340)	Acc@1   0.00 (  8.08)	Acc@5  12.50 ( 36.74)
Epoch: [4][ 50/812]	Time  0.732 ( 0.582)	Data  0.001 ( 0.102)	Loss 3.0304942131042480 (2.9698263944364061)	Acc@1  12.50 (  8.46)	Acc@5  37.50 ( 37.87)
Epoch: [4][ 60/812]	Time  1.093 ( 0.585)	Data  0.000 ( 0.085)	Loss 3.0427181720733643 (2.9605909722750305)	Acc@1  12.50 (  9.63)	Acc@5  37.50 ( 38.22)
Epoch: [4][ 70/812]	Time  0.397 ( 0.600)	Data  0.000 ( 0.073)	Loss 3.1018717288970947 (2.9560941944659596)	Acc@1   6.25 (  9.33)	Acc@5  37.50 ( 37.85)
Epoch: [4][ 80/812]	Time  0.323 ( 0.634)	Data  0.000 ( 0.064)	Loss 3.1076812744140625 (2.9614356859230702)	Acc@1   6.25 (  9.34)	Acc@5  50.00 ( 38.04)
Epoch: [4][ 90/812]	Time  0.560 ( 0.602)	Data  0.000 ( 0.057)	Loss 3.0201036930084229 (2.9682986683897918)	Acc@1   0.00 (  8.86)	Acc@5  37.50 ( 36.95)
Epoch: [4][100/812]	Time  0.456 ( 0.579)	Data  0.000 ( 0.052)	Loss 3.0130424499511719 (2.9760245804739469)	Acc@1   0.00 (  8.48)	Acc@5  25.00 ( 36.45)
Epoch: [4][110/812]	Time  0.301 ( 0.601)	Data  0.000 ( 0.047)	Loss 2.9456737041473389 (2.9755095898568094)	Acc@1   6.25 (  8.11)	Acc@5  37.50 ( 36.26)
Epoch: [4][120/812]	Time  0.534 ( 0.613)	Data  0.000 ( 0.043)	Loss 2.8952789306640625 (2.9797194122282926)	Acc@1   6.25 (  7.90)	Acc@5  43.75 ( 35.69)
Epoch: [4][130/812]	Time  0.471 ( 0.656)	Data  0.000 ( 0.040)	Loss 2.8875956535339355 (2.9855034424148443)	Acc@1  12.50 (  7.63)	Acc@5  37.50 ( 34.92)
Epoch: [4][140/812]	Time  0.503 ( 0.639)	Data  0.000 ( 0.037)	Loss 3.1707620620727539 (2.9853390125518149)	Acc@1   0.00 (  7.54)	Acc@5  18.75 ( 34.57)
Epoch: [4][150/812]	Time  1.295 ( 0.649)	Data  0.000 ( 0.035)	Loss 3.0668957233428955 (2.9857732112833997)	Acc@1   6.25 (  7.57)	Acc@5   6.25 ( 34.15)
Epoch: [4][160/812]	Time  0.355 ( 0.635)	Data  0.000 ( 0.033)	Loss 2.8755438327789307 (2.9858703894644791)	Acc@1  12.50 (  7.45)	Acc@5  50.00 ( 33.89)
Epoch: [4][170/812]	Time  0.284 ( 0.617)	Data  0.000 ( 0.031)	Loss 2.9792587757110596 (2.9875302914290400)	Acc@1   6.25 (  7.35)	Acc@5  50.00 ( 33.88)
Epoch: [4][180/812]	Time  1.449 ( 0.617)	Data  0.000 ( 0.029)	Loss 3.1594762802124023 (2.9899182767499219)	Acc@1   6.25 (  7.29)	Acc@5  37.50 ( 33.63)
Epoch: [4][190/812]	Time  0.274 ( 0.620)	Data  0.000 ( 0.028)	Loss 2.8869736194610596 (2.9896725672077760)	Acc@1  12.50 (  7.43)	Acc@5  43.75 ( 33.54)
Epoch: [4][200/812]	Time  0.298 ( 0.606)	Data  0.000 ( 0.026)	Loss 3.3442282676696777 (2.9939373630789383)	Acc@1   6.25 (  7.18)	Acc@5  12.50 ( 33.05)
Epoch: [4][210/812]	Time  2.261 ( 0.646)	Data  0.002 ( 0.025)	Loss 3.0488405227661133 (2.9948488016264134)	Acc@1   0.00 (  7.14)	Acc@5  31.25 ( 32.61)
Epoch: [4][220/812]	Time  0.513 ( 0.657)	Data  0.000 ( 0.024)	Loss 2.9683017730712891 (2.9943409829118135)	Acc@1   6.25 (  6.99)	Acc@5  31.25 ( 32.58)
Epoch: [4][230/812]	Time  0.448 ( 0.648)	Data  0.000 ( 0.023)	Loss 3.1102750301361084 (2.9951554820651101)	Acc@1   0.00 (  6.79)	Acc@5  12.50 ( 32.55)
Epoch: [4][240/812]	Time  0.327 ( 0.634)	Data  0.000 ( 0.022)	Loss 2.9227645397186279 (2.9961951412105954)	Acc@1   0.00 (  6.85)	Acc@5  43.75 ( 32.55)
Epoch: [4][250/812]	Time  0.368 ( 0.624)	Data  0.000 ( 0.021)	Loss 2.8825984001159668 (2.9981709248516188)	Acc@1   6.25 (  6.75)	Acc@5  50.00 ( 32.57)
Epoch: [4][260/812]	Time  1.457 ( 0.626)	Data  0.001 ( 0.020)	Loss 2.9738442897796631 (2.9988083565372161)	Acc@1  12.50 (  6.80)	Acc@5  37.50 ( 32.45)
Epoch: [4][270/812]	Time  0.314 ( 0.623)	Data  0.000 ( 0.020)	Loss 2.9952397346496582 (2.9993441263248122)	Acc@1   0.00 (  6.71)	Acc@5  31.25 ( 32.29)
Epoch: [4][280/812]	Time  0.718 ( 0.621)	Data  0.001 ( 0.019)	Loss 2.9418797492980957 (2.9987666097824262)	Acc@1   0.00 (  6.72)	Acc@5  31.25 ( 32.25)
Epoch: [4][290/812]	Time  2.276 ( 0.644)	Data  0.009 ( 0.018)	Loss 3.0529999732971191 (2.9995474585962461)	Acc@1  18.75 (  6.79)	Acc@5  37.50 ( 32.22)
Epoch: [4][300/812]	Time  0.427 ( 0.639)	Data  0.000 ( 0.018)	Loss 2.9318232536315918 (2.9997118778799061)	Acc@1  12.50 (  6.77)	Acc@5  37.50 ( 32.27)
Epoch: [4][310/812]	Time  0.710 ( 0.633)	Data  0.000 ( 0.017)	Loss 2.9973244667053223 (2.9984615531381689)	Acc@1  25.00 (  6.77)	Acc@5  50.00 ( 32.34)
Epoch: [4][320/812]	Time  0.400 ( 0.641)	Data  0.000 ( 0.017)	Loss 3.0946140289306641 (2.9999897947935299)	Acc@1   6.25 (  6.81)	Acc@5  43.75 ( 32.42)
Epoch: [4][330/812]	Time  0.316 ( 0.631)	Data  0.000 ( 0.016)	Loss 2.9095935821533203 (2.9997252351927974)	Acc@1  18.75 (  6.95)	Acc@5  50.00 ( 32.44)
Epoch: [4][340/812]	Time  0.309 ( 0.623)	Data  0.000 ( 0.016)	Loss 3.1067268848419189 (3.0008560709240149)	Acc@1   0.00 (  6.98)	Acc@5  18.75 ( 32.22)
Epoch: [4][350/812]	Time  0.302 ( 0.614)	Data  0.000 ( 0.015)	Loss 2.8098387718200684 (2.9997517750134155)	Acc@1   6.25 (  7.02)	Acc@5  43.75 ( 32.12)
Epoch: [4][360/812]	Time  0.336 ( 0.606)	Data  0.000 ( 0.015)	Loss 3.2875158786773682 (2.9994523195018399)	Acc@1   0.00 (  7.10)	Acc@5  12.50 ( 32.13)
Epoch: [4][370/812]	Time  0.473 ( 0.602)	Data  0.000 ( 0.014)	Loss 3.1451530456542969 (3.0022241388048445)	Acc@1   6.25 (  7.04)	Acc@5  25.00 ( 32.01)
Epoch: [4][380/812]	Time  0.327 ( 0.595)	Data  0.000 ( 0.014)	Loss 3.0029392242431641 (3.0018072772839566)	Acc@1   0.00 (  6.96)	Acc@5   6.25 ( 31.97)
Epoch: [4][390/812]	Time  0.320 ( 0.588)	Data  0.000 ( 0.014)	Loss 3.0656514167785645 (3.0020594200514772)	Acc@1   0.00 (  6.95)	Acc@5  37.50 ( 31.95)
Epoch: [4][400/812]	Time  1.482 ( 0.585)	Data  0.001 ( 0.013)	Loss 3.0780043601989746 (3.0038688438491632)	Acc@1   0.00 (  6.90)	Acc@5  18.75 ( 31.76)
Epoch: [4][410/812]	Time  0.503 ( 0.602)	Data  0.001 ( 0.013)	Loss 2.9664692878723145 (3.0032022138581658)	Acc@1  12.50 (  6.98)	Acc@5  25.00 ( 31.71)
Epoch: [4][420/812]	Time  0.390 ( 0.598)	Data  0.000 ( 0.013)	Loss 2.7961354255676270 (3.0036682244434494)	Acc@1   6.25 (  6.96)	Acc@5  37.50 ( 31.65)
Epoch: [4][430/812]	Time  0.265 ( 0.606)	Data  0.000 ( 0.013)	Loss 2.9745316505432129 (3.0031720579638557)	Acc@1   0.00 (  6.98)	Acc@5  18.75 ( 31.71)
Epoch: [4][440/812]	Time  0.423 ( 0.600)	Data  0.000 ( 0.012)	Loss 3.1181888580322266 (3.0027417071552231)	Acc@1   6.25 (  6.97)	Acc@5  18.75 ( 31.86)
Epoch: [4][450/812]	Time  0.264 ( 0.593)	Data  0.000 ( 0.012)	Loss 3.0460796356201172 (3.0034398155043238)	Acc@1   0.00 (  6.98)	Acc@5  25.00 ( 32.00)
Epoch: [4][460/812]	Time  0.293 ( 0.587)	Data  0.000 ( 0.012)	Loss 2.9473543167114258 (3.0049184004810523)	Acc@1   0.00 (  6.94)	Acc@5  18.75 ( 31.90)
Epoch: [4][470/812]	Time  0.310 ( 0.582)	Data  0.000 ( 0.011)	Loss 3.0395216941833496 (3.0049706431710796)	Acc@1   0.00 (  6.90)	Acc@5  37.50 ( 31.82)
Epoch: [4][480/812]	Time  0.672 ( 0.592)	Data  0.002 ( 0.011)	Loss 3.0347671508789062 (3.0044904900191973)	Acc@1   6.25 (  6.93)	Acc@5  37.50 ( 31.96)
Epoch: [4][490/812]	Time  0.723 ( 0.597)	Data  0.000 ( 0.011)	Loss 3.1231970787048340 (3.0053771357915067)	Acc@1   0.00 (  6.86)	Acc@5  25.00 ( 31.86)
Epoch: [4][500/812]	Time  0.294 ( 0.608)	Data  0.000 ( 0.011)	Loss 3.0652215480804443 (3.0064002725178609)	Acc@1  12.50 (  6.87)	Acc@5  31.25 ( 31.80)
Epoch: [4][510/812]	Time  0.505 ( 0.602)	Data  0.000 ( 0.011)	Loss 3.0606126785278320 (3.0073584264505167)	Acc@1   0.00 (  6.85)	Acc@5  18.75 ( 31.69)
Epoch: [4][520/812]	Time  1.756 ( 0.605)	Data  0.001 ( 0.010)	Loss 2.9783885478973389 (3.0067975470749753)	Acc@1   6.25 (  6.78)	Acc@5  31.25 ( 31.67)
Epoch: [4][530/812]	Time  0.786 ( 0.614)	Data  0.000 ( 0.010)	Loss 2.9153423309326172 (3.0079046588833047)	Acc@1  18.75 (  6.77)	Acc@5  43.75 ( 31.63)
Epoch: [4][540/812]	Time  3.362 ( 0.618)	Data  0.001 ( 0.010)	Loss 3.0144367218017578 (3.0083820282201010)	Acc@1  12.50 (  6.75)	Acc@5  37.50 ( 31.59)
Epoch: [4][550/812]	Time  0.449 ( 0.622)	Data  0.001 ( 0.010)	Loss 3.1014394760131836 (3.0082293162544929)	Acc@1   6.25 (  6.78)	Acc@5  37.50 ( 31.57)
Epoch: [4][560/812]	Time  0.253 ( 0.616)	Data  0.000 ( 0.010)	Loss 3.1011600494384766 (3.0090267590235475)	Acc@1   6.25 (  6.76)	Acc@5  25.00 ( 31.45)
Epoch: [4][570/812]	Time  0.485 ( 0.612)	Data  0.000 ( 0.010)	Loss 3.1204981803894043 (3.0092183086375219)	Acc@1   0.00 (  6.73)	Acc@5  37.50 ( 31.46)
Epoch: [4][580/812]	Time  0.265 ( 0.609)	Data  0.000 ( 0.009)	Loss 3.0324811935424805 (3.0100254877093735)	Acc@1  12.50 (  6.76)	Acc@5  37.50 ( 31.38)
Epoch: [4][590/812]	Time  0.433 ( 0.619)	Data  0.000 ( 0.009)	Loss 3.0841081142425537 (3.0102542003399226)	Acc@1   0.00 (  6.73)	Acc@5  25.00 ( 31.26)
Epoch: [4][600/812]	Time  0.294 ( 0.614)	Data  0.000 ( 0.009)	Loss 2.9678025245666504 (3.0094569094367509)	Acc@1   6.25 (  6.73)	Acc@5  25.00 ( 31.30)
Epoch: [4][610/812]	Time  0.273 ( 0.609)	Data  0.000 ( 0.009)	Loss 3.1301407814025879 (3.0093177773558373)	Acc@1   6.25 (  6.77)	Acc@5  25.00 ( 31.28)
Epoch: [4][620/812]	Time  0.885 ( 0.615)	Data  0.001 ( 0.009)	Loss 3.1282176971435547 (3.0106185960692868)	Acc@1   6.25 (  6.71)	Acc@5  25.00 ( 31.26)
Epoch: [4][630/812]	Time  0.265 ( 0.617)	Data  0.000 ( 0.009)	Loss 2.9520018100738525 (3.0102250175506300)	Acc@1  18.75 (  6.75)	Acc@5  37.50 ( 31.23)
Epoch: [4][640/812]	Time  0.380 ( 0.612)	Data  0.000 ( 0.009)	Loss 2.9256658554077148 (3.0100848756602701)	Acc@1   6.25 (  6.73)	Acc@5  37.50 ( 31.30)
Epoch: [4][650/812]	Time  0.671 ( 0.609)	Data  0.001 ( 0.008)	Loss 3.1556630134582520 (3.0107964253462223)	Acc@1  12.50 (  6.68)	Acc@5  25.00 ( 31.16)
Epoch: [4][660/812]	Time  0.576 ( 0.611)	Data  0.000 ( 0.008)	Loss 3.3460354804992676 (3.0114349308966148)	Acc@1   0.00 (  6.67)	Acc@5  12.50 ( 31.10)
Epoch: [4][670/812]	Time  3.341 ( 0.619)	Data  0.001 ( 0.008)	Loss 3.0997328758239746 (3.0127151410377149)	Acc@1   6.25 (  6.63)	Acc@5  37.50 ( 30.94)
Epoch: [4][680/812]	Time  0.777 ( 0.619)	Data  0.000 ( 0.008)	Loss 3.0694627761840820 (3.0123885336076284)	Acc@1  18.75 (  6.62)	Acc@5  31.25 ( 30.95)
Epoch: [4][690/812]	Time  0.931 ( 0.623)	Data  0.000 ( 0.008)	Loss 2.8923153877258301 (3.0132107934799968)	Acc@1   6.25 (  6.65)	Acc@5  37.50 ( 30.93)
Epoch: [4][700/812]	Time  0.265 ( 0.624)	Data  0.000 ( 0.008)	Loss 3.1031320095062256 (3.0138307566649565)	Acc@1   0.00 (  6.61)	Acc@5   6.25 ( 30.88)
Epoch: [4][710/812]	Time  0.349 ( 0.619)	Data  0.000 ( 0.008)	Loss 3.1802387237548828 (3.0133409946109007)	Acc@1  12.50 (  6.65)	Acc@5  31.25 ( 30.91)
Epoch: [4][720/812]	Time  0.654 ( 0.616)	Data  0.000 ( 0.008)	Loss 3.0941803455352783 (3.0139208441799128)	Acc@1   6.25 (  6.63)	Acc@5  18.75 ( 30.86)
Epoch: [4][730/812]	Time  0.291 ( 0.625)	Data  0.000 ( 0.008)	Loss 2.9404716491699219 (3.0140312412695094)	Acc@1  12.50 (  6.68)	Acc@5  25.00 ( 30.89)
Epoch: [4][740/812]	Time  0.311 ( 0.620)	Data  0.000 ( 0.008)	Loss 2.9441499710083008 (3.0136370028239674)	Acc@1   6.25 (  6.68)	Acc@5  37.50 ( 30.95)
Epoch: [4][750/812]	Time  0.360 ( 0.616)	Data  0.000 ( 0.007)	Loss 3.2849075794219971 (3.0136690361998211)	Acc@1   0.00 (  6.64)	Acc@5  25.00 ( 30.95)
Epoch: [4][760/812]	Time  0.786 ( 0.621)	Data  0.000 ( 0.007)	Loss 2.8924942016601562 (3.0139005390949225)	Acc@1   6.25 (  6.64)	Acc@5  25.00 ( 30.95)
Epoch: [4][770/812]	Time  0.421 ( 0.619)	Data  0.000 ( 0.007)	Loss 2.9475111961364746 (3.0138695249297740)	Acc@1   0.00 (  6.66)	Acc@5  37.50 ( 30.95)
Epoch: [4][780/812]	Time  0.409 ( 0.617)	Data  0.000 ( 0.007)	Loss 3.1333394050598145 (3.0143984185290855)	Acc@1   0.00 (  6.63)	Acc@5  12.50 ( 30.89)
Epoch: [4][790/812]	Time  0.341 ( 0.621)	Data  0.000 ( 0.007)	Loss 3.1194696426391602 (3.0143521015924390)	Acc@1   6.25 (  6.66)	Acc@5  25.00 ( 30.91)
Epoch: [4][800/812]	Time  0.389 ( 0.617)	Data  0.000 ( 0.007)	Loss 2.8835837841033936 (3.0140111184447593)	Acc@1  12.50 (  6.66)	Acc@5  31.25 ( 30.89)
Epoch: [4][810/812]	Time  1.006 ( 0.621)	Data  0.000 ( 0.007)	Loss 2.9328176975250244 (3.0140821513353528)	Acc@1  12.50 (  6.64)	Acc@5  31.25 ( 30.93)
epoch: 4, Avg_Loss 3.014408987143944
Test: [  0/204]	Time  6.028 ( 6.028)	Loss 2.9905e+00 (2.9905e+00)	Acc@1  12.50 ( 12.50)	Acc@5  18.75 ( 18.75)
Test: [ 10/204]	Time  0.062 ( 0.701)	Loss 3.1161e+00 (3.0061e+00)	Acc@1   6.25 (  8.52)	Acc@5  37.50 ( 31.25)
Test: [ 20/204]	Time  0.068 ( 0.403)	Loss 2.9305e+00 (2.9732e+00)	Acc@1   0.00 (  6.85)	Acc@5  37.50 ( 34.82)
Test: [ 30/204]	Time  0.154 ( 0.314)	Loss 2.9253e+00 (2.9872e+00)	Acc@1   6.25 (  6.45)	Acc@5  37.50 ( 34.27)
Test: [ 40/204]	Time  0.196 ( 0.269)	Loss 2.9938e+00 (2.9768e+00)	Acc@1  18.75 (  7.16)	Acc@5  37.50 ( 34.45)
Test: [ 50/204]	Time  0.173 ( 0.244)	Loss 2.9923e+00 (2.9900e+00)	Acc@1   6.25 (  6.74)	Acc@5  25.00 ( 32.48)
Test: [ 60/204]	Time  0.118 ( 0.223)	Loss 3.0124e+00 (2.9873e+00)	Acc@1   0.00 (  6.45)	Acc@5  31.25 ( 32.48)
Test: [ 70/204]	Time  0.299 ( 0.216)	Loss 2.9384e+00 (2.9923e+00)	Acc@1   6.25 (  6.43)	Acc@5  25.00 ( 32.92)
Test: [ 80/204]	Time  0.249 ( 0.212)	Loss 3.0347e+00 (2.9930e+00)	Acc@1   0.00 (  6.33)	Acc@5  25.00 ( 33.33)
Test: [ 90/204]	Time  0.247 ( 0.216)	Loss 3.0307e+00 (2.9886e+00)	Acc@1   6.25 (  6.73)	Acc@5  31.25 ( 34.13)
Test: [100/204]	Time  0.390 ( 0.219)	Loss 2.9724e+00 (2.9925e+00)	Acc@1   0.00 (  6.50)	Acc@5  25.00 ( 33.48)
Test: [110/204]	Time  0.079 ( 0.225)	Loss 2.8813e+00 (2.9874e+00)	Acc@1  12.50 (  6.53)	Acc@5  18.75 ( 33.33)
Test: [120/204]	Time  0.073 ( 0.213)	Loss 2.8338e+00 (2.9898e+00)	Acc@1   6.25 (  6.40)	Acc@5  56.25 ( 33.16)
Test: [130/204]	Time  0.071 ( 0.203)	Loss 3.0881e+00 (2.9891e+00)	Acc@1   6.25 (  6.39)	Acc@5  31.25 ( 33.11)
Test: [140/204]	Time  0.058 ( 0.195)	Loss 2.9646e+00 (2.9891e+00)	Acc@1   0.00 (  6.21)	Acc@5  18.75 ( 32.80)
Test: [150/204]	Time  0.089 ( 0.190)	Loss 2.9932e+00 (2.9893e+00)	Acc@1   6.25 (  6.25)	Acc@5  31.25 ( 32.82)
Test: [160/204]	Time  0.085 ( 0.184)	Loss 3.1237e+00 (2.9878e+00)	Acc@1   0.00 (  6.33)	Acc@5  18.75 ( 33.07)
Test: [170/204]	Time  0.057 ( 0.178)	Loss 3.2010e+00 (2.9928e+00)	Acc@1   0.00 (  6.14)	Acc@5  25.00 ( 33.00)
Test: [180/204]	Time  0.093 ( 0.173)	Loss 2.9798e+00 (2.9936e+00)	Acc@1   0.00 (  6.15)	Acc@5  43.75 ( 33.15)
Test: [190/204]	Time  0.057 ( 0.168)	Loss 3.0256e+00 (2.9930e+00)	Acc@1   6.25 (  6.38)	Acc@5  25.00 ( 33.25)
Test: [200/204]	Time  0.060 ( 0.163)	Loss 2.9792e+00 (2.9924e+00)	Acc@1  12.50 (  6.47)	Acc@5  31.25 ( 33.30)
 * Acc@1 6.421 Acc@5 33.057
Epoch: [5][  0/812]	Time  3.946 ( 3.946)	Data  3.510 ( 3.510)	Loss 3.0093679428100586 (3.0093679428100586)	Acc@1   6.25 (  6.25)	Acc@5  25.00 ( 25.00)
Epoch: [5][ 10/812]	Time  0.471 ( 0.938)	Data  0.001 ( 0.319)	Loss 3.0481305122375488 (3.0265000299973921)	Acc@1   6.25 (  5.68)	Acc@5  37.50 ( 32.95)
Epoch: [5][ 20/812]	Time  0.331 ( 0.702)	Data  0.000 ( 0.167)	Loss 2.7773935794830322 (3.0202298845563615)	Acc@1  18.75 (  7.14)	Acc@5  62.50 ( 32.44)
Epoch: [5][ 30/812]	Time  0.327 ( 0.581)	Data  0.000 ( 0.114)	Loss 3.0467236042022705 (3.0046687510705765)	Acc@1   6.25 (  8.27)	Acc@5  37.50 ( 34.68)
Epoch: [5][ 40/812]	Time  0.550 ( 0.704)	Data  0.000 ( 0.086)	Loss 2.9817192554473877 (3.0046680206205787)	Acc@1   0.00 (  6.86)	Acc@5  31.25 ( 33.08)
Epoch: [5][ 50/812]	Time  0.423 ( 0.680)	Data  0.000 ( 0.069)	Loss 3.1821658611297607 (3.0189675864051368)	Acc@1   0.00 (  6.37)	Acc@5  18.75 ( 32.23)
Epoch: [5][ 60/812]	Time  0.437 ( 0.624)	Data  0.000 ( 0.058)	Loss 2.9604375362396240 (3.0223497679976168)	Acc@1   6.25 (  6.05)	Acc@5  18.75 ( 31.56)
Epoch: [5][ 70/812]	Time  1.255 ( 0.730)	Data  0.000 ( 0.050)	Loss 3.0259866714477539 (3.0223120769984284)	Acc@1  12.50 (  6.43)	Acc@5  25.00 ( 31.25)
Epoch: [5][ 80/812]	Time  0.407 ( 0.690)	Data  0.000 ( 0.044)	Loss 2.8515977859497070 (3.0183455296504644)	Acc@1  12.50 (  6.48)	Acc@5  43.75 ( 30.79)
Epoch: [5][ 90/812]	Time  0.273 ( 0.700)	Data  0.000 ( 0.039)	Loss 2.9472661018371582 (3.0167118848025143)	Acc@1   0.00 (  6.32)	Acc@5  18.75 ( 30.29)
Epoch: [5][100/812]	Time  0.565 ( 0.680)	Data  0.000 ( 0.035)	Loss 3.0555751323699951 (3.0193786951574948)	Acc@1   6.25 (  6.13)	Acc@5  31.25 ( 29.89)
Epoch: [5][110/812]	Time  0.406 ( 0.700)	Data  0.000 ( 0.032)	Loss 2.9125914573669434 (3.0114637495161176)	Acc@1   6.25 (  6.31)	Acc@5  25.00 ( 29.28)
Epoch: [5][120/812]	Time  0.516 ( 0.688)	Data  0.000 ( 0.030)	Loss 2.9713950157165527 (3.0151326025813079)	Acc@1   6.25 (  6.25)	Acc@5  43.75 ( 29.18)
Epoch: [5][130/812]	Time  0.681 ( 0.692)	Data  0.000 ( 0.027)	Loss 2.9544787406921387 (3.0147616371853663)	Acc@1   0.00 (  6.15)	Acc@5  31.25 ( 29.10)
Epoch: [5][140/812]	Time  0.322 ( 0.703)	Data  0.000 ( 0.025)	Loss 3.1005187034606934 (3.0177645548015621)	Acc@1   0.00 (  5.98)	Acc@5  37.50 ( 29.21)
Epoch: [5][150/812]	Time  0.302 ( 0.676)	Data  0.000 ( 0.024)	Loss 3.0081593990325928 (3.0195102628493151)	Acc@1   0.00 (  5.84)	Acc@5  18.75 ( 29.47)
Epoch: [5][160/812]	Time  0.471 ( 0.657)	Data  0.004 ( 0.022)	Loss 3.0505189895629883 (3.0174867988373180)	Acc@1   6.25 (  6.06)	Acc@5  25.00 ( 30.12)
Epoch: [5][170/812]	Time  2.182 ( 0.676)	Data  0.001 ( 0.021)	Loss 2.9751822948455811 (3.0165472365262214)	Acc@1   0.00 (  6.21)	Acc@5  31.25 ( 30.12)
Epoch: [5][180/812]	Time  0.482 ( 0.687)	Data  0.000 ( 0.020)	Loss 3.0134758949279785 (3.0176739495103528)	Acc@1   6.25 (  6.32)	Acc@5  25.00 ( 30.25)
Epoch: [5][190/812]	Time  0.361 ( 0.683)	Data  0.000 ( 0.019)	Loss 2.9442028999328613 (3.0186994375358700)	Acc@1   0.00 (  6.48)	Acc@5  25.00 ( 30.30)
Epoch: [5][200/812]	Time  0.429 ( 0.704)	Data  0.000 ( 0.018)	Loss 3.0329945087432861 (3.0194457632985281)	Acc@1  12.50 (  6.50)	Acc@5  37.50 ( 30.32)
Epoch: [5][210/812]	Time  0.431 ( 0.689)	Data  0.000 ( 0.017)	Loss 2.9958405494689941 (3.0200189414182543)	Acc@1   0.00 (  6.55)	Acc@5  12.50 ( 30.21)
Epoch: [5][220/812]	Time  0.340 ( 0.671)	Data  0.000 ( 0.017)	Loss 3.0080764293670654 (3.0186792574317205)	Acc@1   6.25 (  6.45)	Acc@5  50.00 ( 30.35)
Epoch: [5][230/812]	Time  0.781 ( 0.659)	Data  0.000 ( 0.016)	Loss 3.0005819797515869 (3.0160307977106666)	Acc@1   6.25 (  6.44)	Acc@5   6.25 ( 30.49)
Epoch: [5][240/812]	Time  0.329 ( 0.668)	Data  0.000 ( 0.015)	Loss 2.8407721519470215 (3.0149406921814093)	Acc@1  25.00 (  6.51)	Acc@5  37.50 ( 30.63)
Epoch: [5][250/812]	Time  0.560 ( 0.657)	Data  0.000 ( 0.015)	Loss 2.9465897083282471 (3.0147142837721987)	Acc@1  12.50 (  6.50)	Acc@5  37.50 ( 30.60)
Epoch: [5][260/812]	Time  0.286 ( 0.663)	Data  0.000 ( 0.014)	Loss 2.8931150436401367 (3.0147331482605915)	Acc@1   6.25 (  6.54)	Acc@5  31.25 ( 30.75)
Epoch: [5][270/812]	Time  0.379 ( 0.650)	Data  0.000 ( 0.014)	Loss 2.8389356136322021 (3.0114590602607305)	Acc@1  12.50 (  6.64)	Acc@5  43.75 ( 31.18)
Epoch: [5][280/812]	Time  0.413 ( 0.640)	Data  0.008 ( 0.013)	Loss 2.8945748805999756 (3.0082990826236822)	Acc@1   6.25 (  6.87)	Acc@5  50.00 ( 31.34)
Epoch: [5][290/812]	Time  0.537 ( 0.658)	Data  0.006 ( 0.013)	Loss 3.0657072067260742 (3.0072571035103288)	Acc@1   6.25 (  6.87)	Acc@5  37.50 ( 31.55)
Epoch: [5][300/812]	Time  0.267 ( 0.648)	Data  0.000 ( 0.013)	Loss 2.8353662490844727 (3.0049090884452641)	Acc@1   6.25 (  6.89)	Acc@5  43.75 ( 31.87)
Epoch: [5][310/812]	Time  0.608 ( 0.643)	Data  0.000 ( 0.012)	Loss 2.8550546169281006 (3.0019589206413442)	Acc@1  18.75 (  6.99)	Acc@5  50.00 ( 32.15)
Epoch: [5][320/812]	Time  0.435 ( 0.645)	Data  0.000 ( 0.012)	Loss 3.1350743770599365 (3.0021105763325440)	Acc@1   0.00 (  7.05)	Acc@5  18.75 ( 32.22)
Epoch: [5][330/812]	Time  0.282 ( 0.640)	Data  0.000 ( 0.011)	Loss 2.9830946922302246 (3.0018066495566931)	Acc@1   0.00 (  7.06)	Acc@5  56.25 ( 32.33)
Epoch: [5][340/812]	Time  0.651 ( 0.631)	Data  0.000 ( 0.011)	Loss 3.0603618621826172 (3.0012812369729764)	Acc@1   0.00 (  7.11)	Acc@5  18.75 ( 32.44)
Epoch: [5][350/812]	Time  0.803 ( 0.652)	Data  0.000 ( 0.011)	Loss 2.9516880512237549 (2.9987030334961720)	Acc@1   0.00 (  7.09)	Acc@5  25.00 ( 32.66)
Epoch: [5][360/812]	Time  0.301 ( 0.654)	Data  0.000 ( 0.011)	Loss 2.9784801006317139 (2.9980313110879915)	Acc@1  12.50 (  7.13)	Acc@5  25.00 ( 32.72)
Epoch: [5][370/812]	Time  0.309 ( 0.659)	Data  0.000 ( 0.010)	Loss 3.0477964878082275 (2.9980003910887274)	Acc@1   6.25 (  7.16)	Acc@5  25.00 ( 32.68)
Epoch: [5][380/812]	Time  0.307 ( 0.649)	Data  0.000 ( 0.010)	Loss 3.1004064083099365 (2.9982287182895528)	Acc@1   0.00 (  7.22)	Acc@5  31.25 ( 32.74)
Epoch: [5][390/812]	Time  0.256 ( 0.640)	Data  0.000 ( 0.010)	Loss 2.9659256935119629 (2.9974346185279321)	Acc@1  18.75 (  7.24)	Acc@5  25.00 ( 32.80)
Epoch: [5][400/812]	Time  0.987 ( 0.635)	Data  0.000 ( 0.010)	Loss 2.8744053840637207 (2.9961678012648130)	Acc@1   6.25 (  7.28)	Acc@5  50.00 ( 33.06)
Epoch: [5][410/812]	Time  0.443 ( 0.641)	Data  0.000 ( 0.009)	Loss 2.8779723644256592 (2.9966767153310658)	Acc@1  18.75 (  7.30)	Acc@5  37.50 ( 33.01)
Epoch: [5][420/812]	Time  0.288 ( 0.632)	Data  0.000 ( 0.009)	Loss 2.9046895503997803 (2.9964520262992296)	Acc@1   6.25 (  7.30)	Acc@5  37.50 ( 33.06)
Epoch: [5][430/812]	Time  0.544 ( 0.626)	Data  0.000 ( 0.009)	Loss 2.9143850803375244 (2.9959969581418138)	Acc@1   0.00 (  7.38)	Acc@5  31.25 ( 33.15)
Epoch: [5][440/812]	Time  0.552 ( 0.622)	Data  0.000 ( 0.009)	Loss 2.8223528861999512 (2.9951365675245012)	Acc@1  12.50 (  7.40)	Acc@5  31.25 ( 33.23)
Epoch: [5][450/812]	Time  1.253 ( 0.627)	Data  0.000 ( 0.009)	Loss 2.7783966064453125 (2.9922142092245911)	Acc@1  12.50 (  7.46)	Acc@5  50.00 ( 33.45)
Epoch: [5][460/812]	Time  0.814 ( 0.634)	Data  0.000 ( 0.008)	Loss 2.8811240196228027 (2.9920350610563400)	Acc@1   6.25 (  7.54)	Acc@5  37.50 ( 33.45)
Epoch: [5][470/812]	Time  0.330 ( 0.635)	Data  0.000 ( 0.008)	Loss 2.8977832794189453 (2.9907639699883268)	Acc@1   0.00 (  7.51)	Acc@5  37.50 ( 33.53)
Epoch: [5][480/812]	Time  0.590 ( 0.631)	Data  0.001 ( 0.008)	Loss 2.9845321178436279 (2.9897155940160931)	Acc@1   6.25 (  7.59)	Acc@5  25.00 ( 33.68)
Epoch: [5][490/812]	Time  0.506 ( 0.627)	Data  0.000 ( 0.008)	Loss 3.0231924057006836 (2.9885929067848656)	Acc@1   6.25 (  7.61)	Acc@5  31.25 ( 33.83)
Epoch: [5][500/812]	Time  0.580 ( 0.646)	Data  0.001 ( 0.008)	Loss 3.2336368560791016 (2.9886464073272521)	Acc@1   0.00 (  7.62)	Acc@5  18.75 ( 33.78)
Epoch: [5][510/812]	Time  0.323 ( 0.641)	Data  0.000 ( 0.008)	Loss 3.1464917659759521 (2.9886755332088284)	Acc@1   0.00 (  7.58)	Acc@5  18.75 ( 33.87)
Epoch: [5][520/812]	Time  0.256 ( 0.636)	Data  0.000 ( 0.008)	Loss 2.9964511394500732 (2.9886394338735922)	Acc@1  25.00 (  7.59)	Acc@5  37.50 ( 33.93)
Epoch: [5][530/812]	Time  0.343 ( 0.630)	Data  0.002 ( 0.007)	Loss 2.8239517211914062 (2.9883203466059798)	Acc@1   0.00 (  7.56)	Acc@5  43.75 ( 34.00)
Epoch: [5][540/812]	Time  0.545 ( 0.626)	Data  0.000 ( 0.007)	Loss 2.9298131465911865 (2.9889331071081533)	Acc@1   0.00 (  7.56)	Acc@5  37.50 ( 33.98)
Epoch: [5][550/812]	Time  0.309 ( 0.631)	Data  0.000 ( 0.007)	Loss 2.9403829574584961 (2.9887939256678475)	Acc@1   6.25 (  7.53)	Acc@5  43.75 ( 33.93)
Epoch: [5][560/812]	Time  0.342 ( 0.624)	Data  0.000 ( 0.007)	Loss 2.9599430561065674 (2.9879314946193323)	Acc@1   6.25 (  7.58)	Acc@5  25.00 ( 34.00)
Epoch: [5][570/812]	Time  1.028 ( 0.622)	Data  0.000 ( 0.007)	Loss 2.8728566169738770 (2.9868179069091476)	Acc@1  12.50 (  7.65)	Acc@5  50.00 ( 34.08)
Epoch: [5][580/812]	Time  0.437 ( 0.640)	Data  0.000 ( 0.007)	Loss 3.2260389328002930 (2.9866926272026233)	Acc@1   6.25 (  7.68)	Acc@5  18.75 ( 34.12)
Epoch: [5][590/812]	Time  0.670 ( 0.635)	Data  0.000 ( 0.007)	Loss 2.8193879127502441 (2.9854379539360654)	Acc@1   6.25 (  7.71)	Acc@5  37.50 ( 34.26)
Epoch: [5][600/812]	Time  0.378 ( 0.634)	Data  0.000 ( 0.007)	Loss 2.8252491950988770 (2.9836225886511527)	Acc@1   6.25 (  7.75)	Acc@5  43.75 ( 34.32)
Epoch: [5][610/812]	Time  0.616 ( 0.640)	Data  0.000 ( 0.007)	Loss 2.8460249900817871 (2.9830716066001481)	Acc@1   0.00 (  7.78)	Acc@5  37.50 ( 34.49)
Epoch: [5][620/812]	Time  0.258 ( 0.640)	Data  0.000 ( 0.006)	Loss 2.8605425357818604 (2.9831271344336909)	Acc@1   0.00 (  7.79)	Acc@5  43.75 ( 34.44)
Epoch: [5][630/812]	Time  0.328 ( 0.635)	Data  0.000 ( 0.006)	Loss 3.1257233619689941 (2.9822550590744865)	Acc@1   6.25 (  7.84)	Acc@5  25.00 ( 34.56)
Epoch: [5][640/812]	Time  0.283 ( 0.630)	Data  0.000 ( 0.006)	Loss 2.9502015113830566 (2.9810611840901249)	Acc@1  12.50 (  7.89)	Acc@5  43.75 ( 34.65)
Epoch: [5][650/812]	Time  0.481 ( 0.626)	Data  0.000 ( 0.006)	Loss 3.0802688598632812 (2.9798460211805118)	Acc@1   0.00 (  7.95)	Acc@5  50.00 ( 34.80)
Epoch: [5][660/812]	Time  0.790 ( 0.625)	Data  0.000 ( 0.006)	Loss 2.8965363502502441 (2.9781723913374538)	Acc@1   6.25 (  7.94)	Acc@5  37.50 ( 34.87)
Epoch: [5][670/812]	Time  0.668 ( 0.627)	Data  0.001 ( 0.006)	Loss 2.9430878162384033 (2.9785738524310457)	Acc@1   6.25 (  7.95)	Acc@5  31.25 ( 34.86)
Epoch: [5][680/812]	Time  0.378 ( 0.625)	Data  0.000 ( 0.006)	Loss 3.0159733295440674 (2.9773827145278191)	Acc@1   6.25 (  7.96)	Acc@5  50.00 ( 35.02)
Epoch: [5][690/812]	Time  0.265 ( 0.628)	Data  0.000 ( 0.006)	Loss 3.0972957611083984 (2.9768409856666533)	Acc@1   6.25 (  7.95)	Acc@5  50.00 ( 35.14)
Epoch: [5][700/812]	Time  0.555 ( 0.624)	Data  0.000 ( 0.006)	Loss 3.0554809570312500 (2.9754272779282420)	Acc@1   0.00 (  7.95)	Acc@5  31.25 ( 35.21)
Epoch: [5][710/812]	Time  0.738 ( 0.623)	Data  0.001 ( 0.006)	Loss 3.0708472728729248 (2.9766157709596528)	Acc@1   0.00 (  7.93)	Acc@5  18.75 ( 35.18)
Epoch: [5][720/812]	Time  0.781 ( 0.624)	Data  0.000 ( 0.006)	Loss 2.6101152896881104 (2.9751821510008072)	Acc@1  18.75 (  7.99)	Acc@5  56.25 ( 35.22)
Epoch: [5][730/812]	Time  0.601 ( 0.624)	Data  0.000 ( 0.006)	Loss 3.0163030624389648 (2.9744383359248920)	Acc@1   6.25 (  8.03)	Acc@5  25.00 ( 35.29)
Epoch: [5][740/812]	Time  0.786 ( 0.624)	Data  0.000 ( 0.005)	Loss 2.9593212604522705 (2.9737988654096759)	Acc@1   0.00 (  8.04)	Acc@5  31.25 ( 35.26)
Epoch: [5][750/812]	Time  0.408 ( 0.626)	Data  0.000 ( 0.005)	Loss 2.9499056339263916 (2.9724438139665303)	Acc@1  12.50 (  8.02)	Acc@5  43.75 ( 35.29)
Epoch: [5][760/812]	Time  0.418 ( 0.624)	Data  0.000 ( 0.005)	Loss 2.7736265659332275 (2.9726090735110913)	Acc@1  12.50 (  8.01)	Acc@5  37.50 ( 35.32)
Epoch: [5][770/812]	Time  1.066 ( 0.624)	Data  0.000 ( 0.005)	Loss 2.8660213947296143 (2.9721486132432515)	Acc@1   6.25 (  7.98)	Acc@5  56.25 ( 35.34)
Epoch: [5][780/812]	Time  5.541 ( 0.630)	Data  0.000 ( 0.005)	Loss 2.8747820854187012 (2.9714059741182610)	Acc@1   6.25 (  8.03)	Acc@5  43.75 ( 35.43)
Epoch: [5][790/812]	Time  0.267 ( 0.629)	Data  0.000 ( 0.005)	Loss 2.8215460777282715 (2.9704907479991505)	Acc@1   6.25 (  8.08)	Acc@5  43.75 ( 35.45)
Epoch: [5][800/812]	Time  0.369 ( 0.625)	Data  0.000 ( 0.005)	Loss 2.8916840553283691 (2.9698238366849710)	Acc@1   6.25 (  8.10)	Acc@5  43.75 ( 35.51)
Epoch: [5][810/812]	Time  0.358 ( 0.622)	Data  0.000 ( 0.005)	Loss 2.6825907230377197 (2.9692925974708304)	Acc@1   6.25 (  8.14)	Acc@5  50.00 ( 35.62)
epoch: 5, Avg_Loss 2.9705574926484393
Test: [  0/204]	Time  3.367 ( 3.367)	Loss 3.2778e+00 (3.2778e+00)	Acc@1   0.00 (  0.00)	Acc@5  18.75 ( 18.75)
Test: [ 10/204]	Time  0.058 ( 0.628)	Loss 2.8881e+00 (3.1238e+00)	Acc@1   6.25 (  6.25)	Acc@5  43.75 ( 30.68)
Test: [ 20/204]	Time  0.096 ( 0.365)	Loss 3.5762e+00 (3.0741e+00)	Acc@1   6.25 (  5.95)	Acc@5  25.00 ( 30.06)
Test: [ 30/204]	Time  0.113 ( 0.278)	Loss 2.9359e+00 (3.0249e+00)	Acc@1   6.25 (  5.85)	Acc@5  31.25 ( 32.86)
Test: [ 40/204]	Time  0.082 ( 0.234)	Loss 2.9506e+00 (3.0020e+00)	Acc@1   6.25 (  6.40)	Acc@5  25.00 ( 33.08)
Test: [ 50/204]	Time  0.079 ( 0.218)	Loss 2.7955e+00 (2.9933e+00)	Acc@1  18.75 (  6.37)	Acc@5  25.00 ( 32.60)
Test: [ 60/204]	Time  0.079 ( 0.199)	Loss 2.7870e+00 (3.0077e+00)	Acc@1   6.25 (  6.76)	Acc@5  25.00 ( 31.76)
Test: [ 70/204]	Time  0.080 ( 0.181)	Loss 2.9314e+00 (2.9974e+00)	Acc@1  12.50 (  6.87)	Acc@5  25.00 ( 32.04)
Test: [ 80/204]	Time  0.127 ( 0.170)	Loss 2.9055e+00 (3.0098e+00)	Acc@1   6.25 (  6.87)	Acc@5  18.75 ( 32.18)
Test: [ 90/204]	Time  0.073 ( 0.164)	Loss 3.2443e+00 (3.0242e+00)	Acc@1   0.00 (  7.07)	Acc@5  31.25 ( 32.35)
Test: [100/204]	Time  0.063 ( 0.155)	Loss 3.0695e+00 (3.0244e+00)	Acc@1  18.75 (  7.12)	Acc@5  31.25 ( 32.92)
Test: [110/204]	Time  0.111 ( 0.149)	Loss 2.9991e+00 (3.0142e+00)	Acc@1   0.00 (  7.09)	Acc@5  31.25 ( 33.45)
Test: [120/204]	Time  0.138 ( 0.153)	Loss 2.9548e+00 (3.0171e+00)	Acc@1   0.00 (  7.13)	Acc@5  31.25 ( 33.68)
Test: [130/204]	Time  0.084 ( 0.149)	Loss 3.9011e+00 (3.0241e+00)	Acc@1   0.00 (  7.30)	Acc@5   6.25 ( 33.87)
Test: [140/204]	Time  0.091 ( 0.144)	Loss 3.0815e+00 (3.0264e+00)	Acc@1   6.25 (  7.05)	Acc@5  25.00 ( 33.38)
Test: [150/204]	Time  0.333 ( 0.151)	Loss 3.0284e+00 (3.0208e+00)	Acc@1   6.25 (  7.00)	Acc@5  50.00 ( 33.44)
Test: [160/204]	Time  0.143 ( 0.161)	Loss 2.9996e+00 (3.0147e+00)	Acc@1  12.50 (  7.18)	Acc@5  25.00 ( 33.58)
Test: [170/204]	Time  0.146 ( 0.158)	Loss 3.3165e+00 (3.0158e+00)	Acc@1   0.00 (  7.05)	Acc@5  12.50 ( 33.48)
Test: [180/204]	Time  0.339 ( 0.160)	Loss 2.7605e+00 (3.0155e+00)	Acc@1  12.50 (  6.98)	Acc@5  56.25 ( 33.81)
Test: [190/204]	Time  0.300 ( 0.188)	Loss 2.8628e+00 (3.0126e+00)	Acc@1   6.25 (  7.04)	Acc@5  25.00 ( 33.97)
Test: [200/204]	Time  0.185 ( 0.188)	Loss 2.9005e+00 (3.0110e+00)	Acc@1   6.25 (  7.15)	Acc@5  31.25 ( 33.89)
 * Acc@1 7.189 Acc@5 34.009
Epoch: [6][  0/812]	Time  4.394 ( 4.394)	Data  3.987 ( 3.987)	Loss 3.1097726821899414 (3.1097726821899414)	Acc@1   6.25 (  6.25)	Acc@5  31.25 ( 31.25)
Epoch: [6][ 10/812]	Time  0.382 ( 0.812)	Data  0.001 ( 0.363)	Loss 2.8286151885986328 (2.9571313424543901)	Acc@1   6.25 ( 10.23)	Acc@5  37.50 ( 34.09)
Epoch: [6][ 20/812]	Time  0.319 ( 0.580)	Data  0.000 ( 0.190)	Loss 2.6891622543334961 (2.9058533850170316)	Acc@1  12.50 ( 11.61)	Acc@5  56.25 ( 38.39)
Epoch: [6][ 30/812]	Time  0.301 ( 0.685)	Data  0.000 ( 0.129)	Loss 3.0065009593963623 (2.9212702397377259)	Acc@1   6.25 ( 10.28)	Acc@5  37.50 ( 39.31)
Epoch: [6][ 40/812]	Time  0.275 ( 0.591)	Data  0.000 ( 0.098)	Loss 2.8098449707031250 (2.9235785763438153)	Acc@1  12.50 (  9.76)	Acc@5  43.75 ( 38.72)
Epoch: [6][ 50/812]	Time  0.264 ( 0.544)	Data  0.000 ( 0.079)	Loss 2.8396604061126709 (2.9227891342312682)	Acc@1   6.25 (  9.56)	Acc@5  50.00 ( 39.46)
Epoch: [6][ 60/812]	Time  0.380 ( 0.509)	Data  0.000 ( 0.066)	Loss 2.9119305610656738 (2.9267428233975270)	Acc@1   0.00 (  9.32)	Acc@5  31.25 ( 39.24)
Epoch: [6][ 70/812]	Time  0.496 ( 0.524)	Data  0.000 ( 0.057)	Loss 3.2335727214813232 (2.9249337391114572)	Acc@1  12.50 (  9.42)	Acc@5  37.50 ( 39.52)
Epoch: [6][ 80/812]	Time  0.441 ( 0.605)	Data  0.000 ( 0.050)	Loss 2.9235696792602539 (2.9269495275285511)	Acc@1   6.25 (  9.03)	Acc@5  31.25 ( 38.81)
Epoch: [6][ 90/812]	Time  0.389 ( 0.583)	Data  0.000 ( 0.044)	Loss 2.9944047927856445 (2.9268878737648767)	Acc@1   6.25 (  9.07)	Acc@5  43.75 ( 38.80)
Epoch: [6][100/812]	Time  1.136 ( 0.582)	Data  0.000 ( 0.040)	Loss 2.9324245452880859 (2.9233533273829093)	Acc@1   0.00 (  8.73)	Acc@5  37.50 ( 39.11)
Epoch: [6][110/812]	Time  0.254 ( 0.601)	Data  0.000 ( 0.036)	Loss 2.8848965167999268 (2.9230901829831235)	Acc@1  12.50 (  8.45)	Acc@5  43.75 ( 39.02)
Epoch: [6][120/812]	Time  0.316 ( 0.578)	Data  0.000 ( 0.034)	Loss 3.0579926967620850 (2.9228277994581489)	Acc@1   0.00 (  8.06)	Acc@5  18.75 ( 38.79)
Epoch: [6][130/812]	Time  0.284 ( 0.557)	Data  0.000 ( 0.031)	Loss 2.8948912620544434 (2.9202509017391058)	Acc@1  12.50 (  8.25)	Acc@5  50.00 ( 38.84)
Epoch: [6][140/812]	Time  0.627 ( 0.544)	Data  0.000 ( 0.029)	Loss 2.9069974422454834 (2.9195613522901604)	Acc@1  18.75 (  8.51)	Acc@5  37.50 ( 38.56)
Epoch: [6][150/812]	Time  0.341 ( 0.575)	Data  0.000 ( 0.027)	Loss 2.7576642036437988 (2.9176479601702154)	Acc@1  18.75 (  8.65)	Acc@5  43.75 ( 38.53)
Epoch: [6][160/812]	Time  0.435 ( 0.560)	Data  0.000 ( 0.025)	Loss 2.7376990318298340 (2.9201917278100247)	Acc@1   0.00 (  8.46)	Acc@5  37.50 ( 38.08)
Epoch: [6][170/812]	Time  1.346 ( 0.569)	Data  0.001 ( 0.024)	Loss 2.7744154930114746 (2.9201070913794447)	Acc@1   0.00 (  8.30)	Acc@5  56.25 ( 38.63)
Epoch: [6][180/812]	Time  0.493 ( 0.569)	Data  0.001 ( 0.023)	Loss 2.9062271118164062 (2.9196163962559147)	Acc@1   0.00 (  8.32)	Acc@5  25.00 ( 38.74)
Epoch: [6][190/812]	Time  0.490 ( 0.566)	Data  0.000 ( 0.021)	Loss 3.0276436805725098 (2.9207403772164389)	Acc@1   6.25 (  8.34)	Acc@5  31.25 ( 38.68)
Epoch: [6][200/812]	Time  0.506 ( 0.570)	Data  0.000 ( 0.020)	Loss 2.7334020137786865 (2.9172517031579468)	Acc@1  12.50 (  8.55)	Acc@5  37.50 ( 39.05)
Epoch: [6][210/812]	Time  0.333 ( 0.588)	Data  0.000 ( 0.019)	Loss 2.9777569770812988 (2.9177696783960712)	Acc@1   6.25 (  8.56)	Acc@5  31.25 ( 39.13)
Epoch: [6][220/812]	Time  0.347 ( 0.575)	Data  0.000 ( 0.019)	Loss 2.8254382610321045 (2.9182197713204641)	Acc@1  12.50 (  8.63)	Acc@5  56.25 ( 39.14)
Epoch: [6][230/812]	Time  0.397 ( 0.567)	Data  0.001 ( 0.018)	Loss 3.1852836608886719 (2.9163125098009646)	Acc@1   0.00 (  8.66)	Acc@5  18.75 ( 39.45)
Epoch: [6][240/812]	Time  0.320 ( 0.586)	Data  0.000 ( 0.017)	Loss 2.8278150558471680 (2.9185449691234289)	Acc@1   6.25 (  8.77)	Acc@5  50.00 ( 39.39)
Epoch: [6][250/812]	Time  0.333 ( 0.574)	Data  0.000 ( 0.016)	Loss 3.3060045242309570 (2.9189515142326812)	Acc@1   0.00 (  8.89)	Acc@5  31.25 ( 39.32)
Epoch: [6][260/812]	Time  0.383 ( 0.565)	Data  0.000 ( 0.016)	Loss 3.0197992324829102 (2.9171515232758503)	Acc@1   6.25 (  8.91)	Acc@5  37.50 ( 39.42)
Epoch: [6][270/812]	Time  0.551 ( 0.562)	Data  0.000 ( 0.015)	Loss 3.0585439205169678 (2.9187733616776130)	Acc@1   6.25 (  8.99)	Acc@5  37.50 ( 39.32)
Epoch: [6][280/812]	Time  0.459 ( 0.568)	Data  0.000 ( 0.015)	Loss 2.5891299247741699 (2.9159112332978707)	Acc@1  12.50 (  8.99)	Acc@5  56.25 ( 39.35)
Epoch: [6][290/812]	Time  0.264 ( 0.559)	Data  0.000 ( 0.014)	Loss 2.8457083702087402 (2.9155071134010133)	Acc@1   0.00 (  8.96)	Acc@5  37.50 ( 39.43)
Epoch: [6][300/812]	Time  0.363 ( 0.552)	Data  0.000 ( 0.014)	Loss 3.0651352405548096 (2.9161004584493035)	Acc@1   6.25 (  9.03)	Acc@5  43.75 ( 39.53)
Epoch: [6][310/812]	Time  0.267 ( 0.545)	Data  0.000 ( 0.013)	Loss 3.0608608722686768 (2.9150365134910756)	Acc@1   6.25 (  9.04)	Acc@5  31.25 ( 39.63)
Epoch: [6][320/812]	Time  0.315 ( 0.538)	Data  0.000 ( 0.013)	Loss 2.8378710746765137 (2.9169917529988512)	Acc@1   0.00 (  9.07)	Acc@5  50.00 ( 39.68)
Epoch: [6][330/812]	Time  0.278 ( 0.533)	Data  0.000 ( 0.013)	Loss 2.9029331207275391 (2.9187810118464905)	Acc@1  25.00 (  9.03)	Acc@5  43.75 ( 39.65)
Epoch: [6][340/812]	Time  0.330 ( 0.526)	Data  0.000 ( 0.012)	Loss 3.0335958003997803 (2.9199019198543517)	Acc@1   0.00 (  9.00)	Acc@5  12.50 ( 39.57)
Epoch: [6][350/812]	Time  0.335 ( 0.521)	Data  0.000 ( 0.012)	Loss 2.8550031185150146 (2.9205595890001694)	Acc@1  12.50 (  9.08)	Acc@5  50.00 ( 39.51)
Epoch: [6][360/812]	Time  0.427 ( 0.524)	Data  0.000 ( 0.012)	Loss 2.8306479454040527 (2.9205521493737385)	Acc@1  12.50 (  9.05)	Acc@5  43.75 ( 39.44)
Epoch: [6][370/812]	Time  0.363 ( 0.526)	Data  0.000 ( 0.011)	Loss 2.9542319774627686 (2.9202887343589188)	Acc@1   6.25 (  8.98)	Acc@5  50.00 ( 39.47)
Epoch: [6][380/812]	Time  0.355 ( 0.521)	Data  0.000 ( 0.011)	Loss 2.8862643241882324 (2.9193610431641104)	Acc@1   6.25 (  8.99)	Acc@5  37.50 ( 39.55)
Epoch: [6][390/812]	Time  0.855 ( 0.520)	Data  0.000 ( 0.011)	Loss 2.9523932933807373 (2.9179683938965466)	Acc@1   0.00 (  9.10)	Acc@5  25.00 ( 39.50)
Epoch: [6][400/812]	Time  0.298 ( 0.522)	Data  0.000 ( 0.011)	Loss 3.0131950378417969 (2.9185388147682323)	Acc@1  12.50 (  9.20)	Acc@5  37.50 ( 39.56)
Epoch: [6][410/812]	Time  0.571 ( 0.522)	Data  0.001 ( 0.010)	Loss 2.7465417385101318 (2.9192084540995946)	Acc@1   6.25 (  9.14)	Acc@5  50.00 ( 39.49)
Epoch: [6][420/812]	Time  0.534 ( 0.527)	Data  0.000 ( 0.010)	Loss 2.7775554656982422 (2.9184231922349002)	Acc@1   0.00 (  9.04)	Acc@5  43.75 ( 39.53)
